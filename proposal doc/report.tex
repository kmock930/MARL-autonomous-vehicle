\documentclass[a4paper,11pt]{article}
\usepackage{dmasproject}
\usepackage{pgfgantt} % For the Gantt chart
\usepackage{geometry} % For page layout
% if you need additional LaTeX packages, add them here

\title{Asymmetric Communication Policies in Multi-Agent Reinforcement Learning for Tethered Robots}
% sort your names alphabetically by last name
\author{
  Kelvin Mock (Student ID: 101363677)
  \\
  Kimia Ramezani (Student ID: 101050878)
  \\
  Chintan Shah (Student ID: 101321493)
  \\
  Supervised by: Dr. Zinovi Rabinovich (Email: \url{Zinovi.Rabinovich@carleton.ca})
}
\date{February 2025}

\begin{document}

\maketitle

\begin{abstract}
Multi-Agent Reinforcement Learning (MARL) has traditionally focused on environments with bidirectional communication, yet real-world applications often face one-way or unreliable transmissions. This study explores asymmetric communication strategies in MARL, specifically for tethered robots operating in a partially observable 2D grid world. The system consists of a Leader (R1) with full map awareness and a Follower (R2) that relies solely on local sensing and R1’s directional signals. A key challenge is ensuring effective coordination despite limited feedback and differing perception capabilities.

To address this, we propose a Multi-Agent PPO (MAPPO) framework combined with Graph Neural Networks (GNNs) for message encoding and a Sparse Attention Mechanism to optimize communication. Our approach is tested against two baseline models: a fully communicative MARL model and a no-communication model, using metrics such as completion rate, navigation efficiency, tether constraint violations, and collision rate.
\end{abstract}

\section{Project Description}

\subsection{Environment Description}

This project involves a 2D grid-based environment filled with obstacles, where two robots, R1 (Leader) and R2 (Follower), must navigate from Point A to Point B while staying within a fixed tethered distance. Since the environment is only partially observable, each robot can only see a limited area around itself.

\subsection{Agent Interaction}

\paragraph{R1 (Leader)} Has access to a full map and a pre-planned navigation path but does not receive any feedback from R2. This agent can only send simple directional commands (go, stop, left, right) to R2.

\paragraph{R2 (Follower)} Has local sensing capabilities (ray-based detection, grid-based occupancy awareness) but cannot see the entire map. It depends entirely on R1’s instructions and its own local observations to decide its movement.

\subsection{Action Space}

Both robots have discrete movement options: \{Up, Down, Left, Right, Stay\}. \\
R1’s movements are based on its independent decision-making and R2’s movements are influenced by both its own perception and the signals received from R1.

\subsection{Tether Constraint} The robots must remain within a predefined maximum distance. If they exceed this limit, they incur a penalty.

\section{Objectives \& Challenges}

\subsection{Agent's Goal}

The main goal of a robot is to reach the goal position (B) from an arbitrary starting position (A) successfully.

\subsection{Agent's Constraints} 
Robots should avoid obstacles and maintain the tether constraint. Moreover, it is important to minimize the number of steps to improve efficiency.

\subsection{Key Challenges} \paragraph{Asymmetric communication} R2 cannot send feedback, making it difficult for R1 to adjust its strategy dynamically.

\paragraph{Different levels of perception} R1 has a global view, while R2 can only see its immediate surroundings. 

\paragraph{} Hence, coordinating movement effectively within these constraints will make an agent achieving its goal challenging. 


\section{Why This Problem Matters?}

This project could potentially be extended to several real-world applications. 

\subsection{Real-World Applications}

\paragraph{Warehouse \& Supply Chain Robotics \cite{swarm-RL-overview}} Refers to Leader-follower robot systems where a primary unit sends commands, and secondary units must interpret and execute them autonomously.

\paragraph{Military \& Search-and-Rescue Operations} Potentially extends to the idea of drones guiding ground robots in disaster zones where the communication is unreliable \cite{D-MARL}.

\paragraph{Autonomous Convoys \cite{MARL-coord-commun-control}} Implies groups of vehicles following a leader, where only the lead vehicle has access to GPS or the satellite data. Those groups of vehicles could employ possibly a multitude of algorithms, namely the sparse communication graph model \cite{scaling-MARL-adaptive-sparese-commun-graph}, which scales up a multi-agent reinforcement learning based robotic system by reducing the communication overhead and at the same time ensuring coordinated convoy movements. Apart from that, those vehicles are likely employed effective collision avoidance mechanisms for safety \cite{bio-collision-avoidance-swarm-deepRL}.

\section{Research Significance} 

Most multi-agent reinforcement learning (MARL) research assumes bidirectional communication, but real-world systems often rely on one-way or unreliable transmissions. This study challenges existing MARL approaches by investigating optimal learning strategies under such constraints.

\section{Baseline Environment \& Algorithms}

\subsection{Baseline Environment} 
\paragraph{Fully Communicative MARL Model} Both R1 and R2 can exchange full-bidirectional messages. It can be used as a benchmark to compare against the one-way communication constraint.

\paragraph{No Communication Model} R2 relies only on local detection which completely ignores messages from R1.

\subsection{Baseline Algorithms} 
\paragraph{Multi-Agent Deep Q-Network (MADQN)} This algorithm uses independent Q-learning for each agent. D-MARL \cite{D-MARL} explores independent Q-learning variants through \textbf{Heterogeneous-Agent Proximal Policy Optimization (HAPPO)} and shared critics.  In our context, R1 learns what signals to send, and R2 learns how to interpret them.
As suggested by D-MARL \cite{D-MARL}, it is a type of decentralized Q-learning strategy where agents decide their own actions independently.

\paragraph{Independent PPO (IPPO)} Inspired by the idea of independent agents \cite{D-MARL} learning decentralized policies within minimal communication, we intend to design a mechanism where both robots are trained independently, and at the same time, R2 make decisions based on the signals received from R1.

\section{Proposed Solution Approach}
\subsection{Learning Algorithms}

Firstly, \textbf{multi-Agent Proximal Policy Optimization (MAPPO)} \cite{NeurIPS2021} allows agents to share experiences while maintaining decentralized decision-making. Then, \textbf{Graph Neural Networks (GNNs)} \cite{lowe2020multiagent} are used for message encoding which helps R2 learn patterns in R1’s signals over time. In addition, a \textbf{Sparse Attention Mechanism} \cite{das2019tarmac} optimizes when R1 should communicate, preventing unnecessary messages.

\subsection{Project Backup Plan} 
If the MARL approach struggles, alternative solutions include:
\paragraph{Hierarchical Reinforcement Learning (HRL)} R1 acts as a high-level planner, while R2 executes specific actions. This is a joint control-and-communication optimization mechanism where decisions are divided between high-level planners and low-level executors. \cite{deepRL-joint-trajectory-communication-IoT} For example, in autonomous systems (like connected vehicles), hierarchical structures are often implied to differentiate route planning (leader) from control execution (follower).

\paragraph{Self-Supervised Learning for Signal Interpretation} If MARL performs poorly, a supervised loss function can be introduced to refine R2’s decision-making. 

\subsection{Testing \& Performance Comparisons}
\subsection{Evaluation Metrics} 
\begin{itemize}
    \item \textbf{Completion Rate}: Percentage of successful goal-reaching attempts.
    \item \textbf{Navigation Efficiency}: Steps taken relative to the optimal path.
    \item \textbf{Tether Constraint Violations}: Number of times the robots exceed the maximum allowed distance.
    \item \textbf{Collision Rate} Number of times R2 collides with obstacles.
\end{itemize}

\subsection{Comparative Baselines} 
\paragraph{No Communication Model} Tests on whether one-way signals improve performance over local sensing alone.

\paragraph{Fully Communicative Model} Measures whether bidirectional communication leads to better results.

\paragraph{Asymmetric Model (Ours)} Evaluates learned policies against both baselines.

\subsection{Validation} 
If our approach improves task completion, reduces collisions, and enhances efficiency under one-way communication constraints, it proves its practical viability.

\section{Research Roadmap} 
The leader is responsible for dividing the subtasks, managing time and preparing the report and analysis for the assigned section among the other team members. 

\section*{Research Roadmap Table}
\vspace{-3mm}
\begin{tabular}{|c|p{5cm}|p{3cm}|c|}
    \hline
    \textbf{Phase} & \textbf{Task} & \textbf{Lead} & \textbf{Time} \\ \hline
    1 & \textbf{Develop a 2D MARL simulation (using VMAS or PettingZoo)}: \newline Train basic PPO agents for R2’s navigation: & Kelvin & Week 1-2 \\ \hline
    2 & \textbf{Train MADQN and MAPPO models}: \newline Integrate one-way communication constraints. & Kimia & Week 3-4 \\ \hline
    3 & \textbf{Evaluate performance against baseline models}: \newline Optimize message encoding and sparse attention mechanisms. & Chintan & Week 5-6 \\ \hline
\end{tabular}

% Gantt Chart
\begin{ganttchart}[
    hgrid,
    vgrid={*1{dotted}},
    y unit chart=0.8cm,
    title height=1,
    bar height=0.6,
    group/.style={fill=blue!30},
    bar/.style={fill=blue},
    milestone/.style={fill=red},
    progress label text={}
]{1}{10} % Weeks from 1 to 10

  % Title for the chart
  \gantttitle{Project Timeline (Weeks)}{10} \\
  \gantttitlelist{1,...,10}{1} \\

  % Phase 1: Simulation & PPO Training
  \ganttgroup{Phase 1: Simulation \& PPO Training (Kelvin)}{1}{3} \\
  \ganttbar{Main Development (Kelvin)}{1}{3} \\
  \ganttbar[bar/.append style={fill=yellow!70}]{Reporting and Documenting (Chintan)}{2}{3} \\
  \ganttbar[bar/.append style={fill=yellow!70}]{Preparation for Phase 2 (Kimia)}{2}{3} \\

  % Phase 2: MADQN/MAPPO Training
  \ganttgroup{Phase 2: MADQN/MAPPO Training \& Integration (Kimia)}{4}{6} \\
  \ganttbar{Main Development (Kimia)}{4}{6} \\
  \ganttbar[bar/.append style={fill=yellow!70}]{Reporting and Documenting (Kelvin)}{2}{3} \\
  \ganttbar[bar/.append style={fill=green!70}]{Integration Testing for Outcomes from Phase 1 (Kelvin)}{6}{7} \\
  \ganttbar[bar/.append style={fill=yellow!70}]{Preparation for Phase 3 (Chintan)}{5}{6} \\

  % Phase 3: Evaluation and Optimization
  \ganttgroup{Phase 3: Evaluation \& Optimization (Chintan)}{7}{9} \\
  \ganttbar{Main Development (Chintan)}{7}{9} \\
  \ganttbar[bar/.append style={fill=yellow!70}]{Reporting and Documenting (Kimia)}{2}{3} \\
  \ganttbar[bar/.append style={fill=green!70}]{Integration Testing for Outcomes from Phase 2 (Kimia)}{6}{7} \\
  \ganttbar[bar/.append style={fill=green!70}]{Iteratively Testing Final Outcomes (Everyone)}{9}{10} \\

\end{ganttchart}

\section*{Future Work (Beyond 6 Weeks)}
\begin{itemize}
    \item \textbf{Scaling to Multi-Agent Navigation \cite{off-beat-MARL}}:  Extending the model to handle convoys of robots following R1.
    \item \textbf{Simulating Real-World Signal Loss \cite{D-MARL}}: Testing how disruptions in communication affect performance.
    \item \textbf{Hardware Deployment on Physical Robots}: Transitioning from simulation to real-world implementation.
\end{itemize}

\section{Implementation consideration}
\begin{itemize}
    \item Computationally feasible using NVIDIA 2080/3060 GPUs.
    \item No real-world sensor data required, fully simulated environment.
    \item Aligns with Project Stubs B \& C by modifying communication structures.
    \item Extends MARL research into realistic asymmetric communication scenarios.
\end{itemize}

% This will print you references, please do not change it.
\printbibliography

\section*{Appendix: Project Stubs}
\subsection*{Project A}
Multiple speakers are aware of their relative position in space. However, the follower/listener does not know this relative positioning and has to learn how to fuse the information from multiple guidance sources in a coherent fashion. Furthermore, the order in which the guidance arrives is not indicative of spatial relationship between the speakers. In fact, the order of these messages is scrambled.
\\
Goal: Create an algorithm that will allow the speakers and the listener to learn how facilitate the listener's task through communication. Demonstrate that either speaker's identity or spacial position is coded in their learned message.

\subsection*{Project B}
Multiple speakers are aware of their relative position in space. The follower/listener does recognise their relative position in space as well, and has to fuse the information from multiple guidance sources in a coherent fashion. Furthermore, the order in which the guidance arrives is fixed and is clearly perceived as coming from a specific speaker. However, random communication failures sometimes block the original message from a speaker (usually one, but not always the same one). The non-blocked speakers can identify this occurence and contribute to the signal-channel of the blocked speaker. The non-blocked speakers make the decision to contribute independently of each other, i.e., there's not central decision maker to decide who and what will contribute. All contributes are superimposed on each other.
\\
Goal: Create an algorithm that will allow the speakers and the listener to learn how facilitate the listener's task through communication. Demonstrate that the speakers' contribution strategies to the blocked-spaker signal-channel are correlated. 

\subsection*{Project C}
Multiple speakers are aware of their relative position in space. The follower/listener does recognise their relative position in space as well, and has to fuse the information from multiple guidance sources in a coherent fashion. Furthermore, the --order-- in which the guidance arrives is fixed, but does not contain speaker id a priori. Random communication failures sometimes block the original message from one or more speakers. Thus, the number of messages that arrive can vary. 
\\
Goal: Create an algorithm that will allow the speakers and the listener to learn how facilitate the listener's task through communication. Demonstrate how the communication strategy and performance depend (if at all) on the probability of communication failure. 

\subsection*{Project D}
Consider a MARL that uses full parameter sharing (swarm-like behaviour), but no communication, and is capable of solving at least one of the following scenarios of VMAS:
\begin{itemize}
    \item balance
    \item wheel
\end{itemize}

Now, modify the scenario so that at random intervals one random agent (not necessarily the same) loses control for 'k' steps -- a freeze event. More specifically, while the "frozen" agent continues to receive its usual perceptions and makes an action decision, the chosen action is not implemented. Instead, a default "zero-accelleration" action takes affect.
\\
Goal: What will be the performance of the solution algorithm under these conditions as a function of the "freeze" probability? Can you modify the algorithm to recover the performance? You can expand the sensory input of the agent to include information about other agents, but this information should be anonymous and non-ordered. 

\subsection*{Project E}
Consider a MARL that uses full parameter sharing (swarm-like behaviour), but no communication, and is capable of solving at least one of the following scenarios of VMAS:
\begin{itemize}
    \item balance
    \item wheel
\end{itemize}

Now, modify the scenario so that at random intervals one random agent (not necessarily the same) loses perception for 'k' steps -- a "blind" event. More specifically, while the "blinded" agent's chosen actions continue to be implemented as usual, the observations it receives are supplanted by a neutral "all-zeros" signal or "white noise".
\\
Goal: What will be the performance of the solution algorithm under these conditions as a function of the "blind" probability? Can you modify the algorithm to recover the performance? You can expand the (non-blinded) sensory input of the agent to include information about other agents, but this information should be anonymous and non-ordered. 

\end{document}
