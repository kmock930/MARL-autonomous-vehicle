# This is the new implementation
# where the leader message size = 8

# -*- coding: utf-8 -*-
"""MARL_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSK3DT299xLss5u2fcOZsYO7y2oa0Pgr
"""

# # Define the Actor (Policy Network)
# class Actor(tf.keras.Model):
#     def __init__(self, num_actions):
#         super(Actor, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(num_actions, activation="softmax")

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

# # Define the Critic (Value Network)
# class Critic(tf.keras.Model):
#     def __init__(self):
#         super(Critic, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(1)

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

import tensorflow as tf
import numpy as np
from constants import ACTION_SPACE, REWARDS, LEADER_MESSAGE_SIZE
# Import the Env
import sys
import os
SIMPLEGRID_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), 'gym-simplegrid', 'gym_simplegrid', 'envs'))
sys.path.append(SIMPLEGRID_PATH)
from simple_grid import SimpleGridEnv
from agent import Agent  # Import the Agent class
import pandas as pd
import datetime

FREE: int = 0
OBSTACLE_SOFT: int = 1
OBSTACLE_HARD: int = 2
AGENT: int = 3
TARGET: int = 4

# Define LEADER and FOLLOWER constants for role-based checks
LEADER = "leader"
FOLLOWER = "follower"

# Initialize the environment
env = SimpleGridEnv(
    render_mode="rgb_array", # numpy array representation
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
)

# Modify the `new_pos` function to check roles using the Agent class
def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE, agents: list):
    x, y = agent_position
    dx, dy = action.value

    new_pos = (x + dx, y + dy)

    # Check if the new position is within the grid
    if not (0 <= new_pos[0] < env.env_configurations["rowSize"] and 0 <= new_pos[1] < env.env_configurations["colSize"]):
        print("Out of Bounds")  # Debugging message
        return agent_position  # Stay

    # Check if the new position is occupied by another agent
    for agent in agents:
        if agent["position"] == new_pos:
            print("Agent Collision")  # Debugging message
            return agent_position  # Stay

    # Check if the new position is a hard obstacle
    if env.obstacles[new_pos[0], new_pos[1]] in [OBSTACLE_HARD]:
        print("Obstacle Collision")  # Debugging message
        return agent_position  # Stay

    print("Valid Move")  # Debugging message
    return new_pos

# Diagonal
# def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE):
#   x,y = agent_position[0]
#   dx,dy = action.value

#   new_pos = (x + dx, y+ dy)

#   if not (0 <= new_pos[0] < 10 and 0 <= new_pos[1] < 10):
#         return agent_position

#   if env[new_pos] in [LEADER, FOLLOWER, OBSTACLE_SOFT, OBSTACLE_HARD]:
#     return agent_position

#   # elif env[new_pos] == 0 or env[new_pos] == 4:
#   #   env[new_pos] == AGENT
#   #   env[agent_position] = FREE

#   return new_pos

# def calculate_reward(env, leader_pos, follower_pos, target):
#   reward = 0
#   if env[leader_pos] == target or env[follower_pos] == target:
#     reward = 50
#   elif env[leader_pos] == OBSTACLE_SOFT or env[follower_pos] == OBSTACLE_SOFT:
#     reward = -0
#   return reward



# - Suggested movement: e.g., (0,0) if leader suggests follower to stay or (0,-1) if suggested going left. Not the coordinates here!
# - Urgency level: int ranging between 1-5, the lower the more urgent.

#window: 3X3
def get_agent_observation(pos: tuple[int, int], env: SimpleGridEnv):
    """
    Generate an observation for the agents based on its position and the environment.
    leader and follower will use this to get their action space. Follower will have 
    additional information about the leader through leader's. 

    Args:
        pos (tuple[int, int]): The position of the leader agent.
        env (SimpleGridEnv): The environment instance.

    Returns:
        list: A message containing information about the environment around the leader.
        
        # Message structure:

        # - Distance to the nearest obstacle (obs_dist): int or float
        # - Whether the path is clear or blocked (path_blocked): 0/1 int
        # - Leader can observe the follower or not (follower_visibility): 0/1 int
        # - Leader's distance to follower (follower_dist): float
        # - Leader's suggested action in x direction (action_dx): int
        # - Leader's suggested action in y direction (action_dy): int
        # - Leader's current x position (x): int
        # - Leader's current y position (y): int

    Author:
        Kimia
    """
    x, y = pos  # Unpack the position
    agent_visibility = 0  # Leader cannot observe the follower initially
    agent_dist, obs_dist, counter = -1, -1, 0
    obstacles_pos, distances = [], []
    path_blocked = 0  # Path is not blocked initially
    xg, yg = (-1, -1)  # Default goal position
    action_dx, action_dy = 0, 0  # Default action

    for dx in range(-2, 3):
        for dy in range(-2, 3):
            nx, ny = x + dx, y + dy
            if 0 <= nx < env.env_configurations["rowSize"] and 0 <= ny < env.env_configurations["colSize"]:
                counter += 1
                # Ensure targets array has the correct dimensions
                if env.targets.shape == (env.env_configurations["rowSize"], env.env_configurations["colSize"]):
                    # Relative position to the goal
                    if env.targets[nx, ny] == env.TARGET:
                        # xg, yg = nx, ny
                        action_dx, action_dy = dx, dy

                # Agents can see each other
                if any(agent['position'] == (nx, ny) and agent.get('role') == 'follower' for agent in env.agents):
                    agent_visibility = 1
                    agent_dist = np.floor(np.sqrt((x - nx) ** 2 + (y - ny) ** 2)) # Round down for diagonal distances

                # Nearest obstacle
                if env.obstacles[nx, ny] in [env.OBSTACLE_SOFT, env.OBSTACLE_HARD]:
                    obstacles_pos.append((nx, ny))
                    dist = np.floor(np.sqrt((x - nx) ** 2 + (y - ny) ** 2)) # Round down for diagonal distances
                    distances.append(dist)

    if len(distances) > 0:
        obs_dist = min(distances)
    if len(obstacles_pos) == counter:
        path_blocked = 1
    
    return [obs_dist, agent_visibility, agent_dist, path_blocked, action_dx, action_dy, dx, dy]

# LSTM
def build_encoder():
    input_layer = tf.keras.layers.Input(shape=(LEADER_MESSAGE_SIZE,))
    reshaped = tf.keras.layers.Reshape((1, LEADER_MESSAGE_SIZE))(input_layer)
    x = tf.keras.layers.LSTM(64, return_sequences=True)(reshaped)
    latent = tf.keras.layers.LSTM(32)(x)
    return tf.keras.models.Model(input_layer, latent, name="encoder")

def build_decoder():
    latent_input = tf.keras.layers.Input(shape=(32,))
    x = tf.keras.layers.RepeatVector(1)(latent_input)
    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)
    x = tf.keras.layers.LSTM(64)(x)
    output_layer = tf.keras.layers.Dense(LEADER_MESSAGE_SIZE, activation="linear")(x)
    return tf.keras.models.Model(latent_input, output_layer, name="decoder")

encoder = build_encoder()
decoder = build_decoder()

# MLP MAPPO
def leader_policy_network():
    input_layer = tf.keras.layers.Input(shape=(LEADER_MESSAGE_SIZE,))
    reshaped = tf.keras.layers.Reshape((1, LEADER_MESSAGE_SIZE))(input_layer)
    # hidden layers
    x = tf.keras.layers.Dense(64, activation="relu")(reshaped)
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    # output layer
    output_layer = tf.keras.layers.Dense(len(ACTION_SPACE), activation="softmax")(x)
    output_layer = tf.keras.layers.Reshape((len(ACTION_SPACE),))(output_layer)
    return tf.keras.models.Model(input_layer, output_layer)

def follower_policy_network():
    # New input shape: 2 entities with 8 features each
    input_layer = tf.keras.layers.Input(shape=(2, LEADER_MESSAGE_SIZE))
    # Pooling layer to aggregate the two entities into one representation.
    # GlobalAveragePooling1D computes the average across the 2 time steps (entities).
    pooled = tf.keras.layers.GlobalAveragePooling1D()(input_layer)
    # Hidden layers remain unchanged
    x = tf.keras.layers.Dense(64, activation="relu")(pooled)
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    # Output layer: use softmax activation to output probabilities over the ACTION_SPACE.
    output_layer = tf.keras.layers.Dense(len(ACTION_SPACE), activation="softmax")(x)
    # Build and return the model
    return tf.keras.models.Model(input_layer, output_layer)

leader_policy = leader_policy_network()
follower_policy = follower_policy_network()

class MAPPO:
    def __init__(self, leader_model, follower_model, encoder, decoder, lr=0.001):
        self.leader_model = leader_model
        self.follower_model = follower_model
        self.encoder = encoder
        self.decoder = decoder
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    def compute_loss(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message, hyperparams: dict = None):
        # Hyperparameters
        contrastive_weight = 0.5  # Default value
        reconstruction_loss_weight = 0.2  # Default value
        entropy_bonus_weight = 0.01  # Default value
        if hyperparams:
            contrastive_weight = hyperparams.get('contrastive_weight', contrastive_weight)
            reconstruction_loss_weight = hyperparams.get('reconstruction_loss_weight', reconstruction_loss_weight)
            entropy_bonus_weight = hyperparams.get('entropy_bonus_weight', entropy_bonus_weight)
        # Convert leader_message and decoded_message to NumPy arrays
        leader_message = np.array(leader_message)
        decoded_message = np.array(decoded_message)

        # Compute Advantage (A = R + Î³V(s') - V(s))
        value = self.leader_model(state_leader.reshape(1, -1))[0, 0]  # Predicted value
        advantage = reward - value  # TD error as Advantage Estimate
        print("loss", advantage)

        # Policy Gradient Loss (A2C)
        action_prob_leader = self.leader_model(state_leader.reshape(1, -1))
        # Combine decoded_msg with leader_message to match the expected input shape
        follower_input = np.stack([decoded_msg, decoded_msg], axis=1)  # Replace the second decoded_msg with follower_leader_message if available
        follower_input = np.reshape(follower_input, (-1, 2, 8))  # Matches the expected input shape
        action_prob_follower = self.follower_model(follower_input)
        policy_loss = -tf.reduce_mean(advantage * tf.math.log(action_prob_leader + 1e-8))
        print('Policy Gradient Loss', policy_loss)
        
        # Contrastive Loss (CACL) for Communication Alignment
        contrastive_loss_value = contrastive_loss(tf.convert_to_tensor([action_prob_follower]), positive_pairs=[0])
        print('Contrastive Loss', contrastive_loss_value)

        # Message Reconstruction Loss (L_recon)
        print(f'leader_message={leader_message}')
        print(f'decoded_message= {decoded_message}')

        # Align shapes of leader_message and decoded_message
        min_dim = min(leader_message.shape[-1], decoded_message.shape[-1])
        leader_message_aligned = leader_message[..., :min_dim]
        decoded_message_aligned = decoded_message[..., :min_dim]

        reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(leader_message_aligned, decoded_message_aligned))
        print('Reconstruction Loss', reconstruction_loss)

        # Entropy Bonus for Exploration
        entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))
        print('Entropy Bonus', entropy_bonus)

        # Final loss function
        total_loss = policy_loss + entropy_bonus_weight * entropy_bonus + contrastive_weight * contrastive_loss_value + reconstruction_loss_weight * reconstruction_loss
        print('Total Loss', total_loss)

        return total_loss


    def apply_gradients(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message):
        with tf.GradientTape() as tape:
            loss = self.compute_loss(
                state_leader=state_leader,
                decoded_msg=decoded_msg,
                action_leader=action_leader,
                action_follower=action_follower,
                reward=reward,
                leader_message=leader_message,
                encoded_message=encoded_message,
                decoded_message=decoded_message
            )
        grads = tape.gradient(loss, self.leader_model.trainable_variables + self.follower_model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.leader_model.trainable_variables + self.follower_model.trainable_variables))

# =======================
# Contrastive Learning for Communication
# =======================
def contrastive_loss(messages, positive_pairs, temperature=0.1):
    """
    Compute the contrastive loss for communication alignment.

    Parameters:
    - messages: Tensor of shape (batch_size, embedding_dim), normalized embeddings.
    - positive_pairs: List of indices representing positive pairs.
    - temperature: Temperature parameter for scaling the similarity matrix.

    Returns:
    - loss: Contrastive loss value.
    """
    # Normalize the embeddings
    messages = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(messages)
    # Compute the similarity matrix
    sim_matrix = tf.matmul(messages, messages, transpose_b=True) / temperature
    sim_matrix = tf.reshape(sim_matrix, (-1, 1))
    # Create one-hot labels for positive pairs
    labels = tf.one_hot(positive_pairs, depth=len(messages))
    labels = tf.reshape(labels, (-1, 1))

    # Compute the binary cross-entropy loss
    print("labels", labels)
    print("sim_matrix", sim_matrix)
    loss = tf.keras.losses.binary_crossentropy(y_true=labels, y_pred=sim_matrix, from_logits=True)
    return tf.reduce_mean(loss)

def train_MAPPO(episodes, leader_model, follower_model, encoder, decoder, env, hyperparams: dict = None, algorithm="MAPPO"):
    print("Starting training...")
    # Logging
    episode_rewards = []
    episode_losses = []
    episode_logs = []  # To store detailed logs for each episode

    # Hyperparameters
    lr = 0.001  # Default learning rate
    max_step_per_episode = 100  # Default max steps per episode
    max_episodes = 100  # Default max episodes
    if hyperparams:
        lr = hyperparams.get('lr', lr)
        max_step_per_episode = hyperparams.get('max_steps', max_step_per_episode)
        max_episodes = hyperparams.get('max_episodes', max_episodes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    total_rewards = []
    success_rate = 0
    collision_count = 0

    # Initialize MAPPO model
    mappo_model = MAPPO(leader_model, follower_model, encoder, decoder, lr)

    episodes = episodes if (episodes is not None or episodes > 0) else max_episodes
    for episode in range(episodes):
        print(f"\nEpisode {episode + 1}/{episodes}")
        # Reset the environment
        obs = env.reset()
        leader_pos = env.leaders[0]['position']
        follower_pos = env.followers[0]['position']

        # Ensure there are targets in the environment
        target_positions = np.argwhere(env.targets == env.TARGET)
        target_pos = target_positions[0] if len(target_positions) > 0 else None
        
        episode_reset = False
        total_reward = 0
        leader_path = [leader_pos]
        follower_path = [follower_pos]

        reward = 0
        tether_violated = 0
        collisions = 0
        distances = []

        reconstruction_loss = 0  # Initialize reconstruction_loss to avoid UnboundLocalError
        entropy_bonus = 0  # Initialize entropy_bonus to avoid UnboundLocalError
        loss = 0  # Initialize loss to avoid UnboundLocalError
        for step in range(max_step_per_episode):  # Limit the number of steps per episode
            # Initialize counters
            steps_taken = 0
            communication_count = 0
            print(f"Step {step + 1}/{max_step_per_episode}")
            # Leader generates a message and takes an action
            print("leader")
            leader_message = get_agent_observation(leader_pos, env)
            communication_count += 1
            leader_message.append(-1)  # Placeholder for additional data if needed
            leader_message.append(-1)  # Placeholder for additional data if needed
            leader_action_probs = leader_model.predict(np.array(leader_message[:LEADER_MESSAGE_SIZE]).reshape(1, -1))
            leader_action = list(ACTION_SPACE)[np.argmax(leader_action_probs)]
            leader_message[4], leader_message[5] = leader_action.value # action_dx, action_dy

            # Update leader position using the step method
            _, _, _, _, info = env.step(
                actions={0: leader_action.value},
                isTraining=True
            )
            new_leader_pos = info['agent_positions'][0]

            # Encode and decode the leader's message
            encoded_msg = encoder.predict(np.array(leader_message[:LEADER_MESSAGE_SIZE]).reshape(1, -1))
            decoded_msg = decoder.predict(encoded_msg)
            decoded_msg = decoded_msg.reshape(1, -1)
            

            # Follower takes an action based on the decoded message
            print("follower")
            follower_env_obs = get_agent_observation(follower_pos, env)
            follower_env_obs.append(-1)  # Placeholder for additional data if needed
            follower_env_obs.append(-1)  # Placeholder for additional data if needed
            
            follower_env_obs = np.array(follower_env_obs[:LEADER_MESSAGE_SIZE]).reshape(1, -1)
            
            # Stack them along a new axis to form an array of shape (1, 2, 8)
            combined_input = np.stack([follower_env_obs, decoded_msg], axis=1)
            # print(f"Follower's Input Shape: {combined_input.shape}")

            follower_action_probs = follower_model.predict(combined_input)
            follower_action = list(ACTION_SPACE)[np.argmax(follower_action_probs)]
            # Update follower position using the step method
            _, _, _, _, info = env.step(
                actions={1: follower_action.value},
                isTraining=True
            )
            new_follower_pos = new_pos(follower_pos, follower_action, env.agents)  # Pass the agents list

            # Compute distance
            distance = np.floor(np.sqrt((new_leader_pos[0] - new_follower_pos[0])**2 + (new_leader_pos[1] - new_follower_pos[1])**2)) # Round down for diagonal distances
            distances.append(distance)

            x_l, y_l = new_leader_pos
            x_f, y_f = new_follower_pos

            # Use tetherDist from the environment configuration
            tether_limit = env.env_configurations["tetherDist"]
            if distance > tether_limit or distance < 1:
                tether_violated += 1
                print(f"Episode {episode+1}: Tether constraint violated (Distance: {distance:.2f}, Tether Limit: {tether_limit}).")
            elif env.obstacles[x_l, y_l] == OBSTACLE_HARD or env.obstacles[x_f, y_f] == OBSTACLE_HARD:
                collisions += 1
                print(f"Episode {episode+1}: Hard obstacle constraint violated. Resetting...")
                break

            # Update the path and position
            for agent in env.agents:
                if agent['position'] == follower_pos:
                    agent['position'] = new_follower_pos
                    break
            follower_pos = new_follower_pos
            follower_path.append(follower_pos)

            for agent in env.agents:
                if agent['position'] == leader_pos:
                    agent['position'] = new_leader_pos
                    break
            leader_pos = new_leader_pos
            leader_path.append(leader_pos)

            # Compute reward
            reward -= 1
            if (0 <= x_l < env.targets.shape[0] and 0 <= y_l < env.targets.shape[1] and env.targets[x_l, y_l] == TARGET) or \
               (0 <= x_f < env.targets.shape[0] and 0 <= y_f < env.targets.shape[1] and env.targets[x_f, y_f] == TARGET):
                reward += REWARDS.TARGET.value
            elif (0 <= x_l < env.obstacles.shape[0] and 0 <= y_l < env.obstacles.shape[1] and env.obstacles[x_l, y_l] == OBSTACLE_SOFT) or \
                 (0 <= x_f < env.obstacles.shape[0] and 0 <= y_f < env.obstacles.shape[1] and env.obstacles[x_f, y_f] == OBSTACLE_SOFT):
                reward += REWARDS.SOFT_OBSTACLE.value
            elif not (0 <= x_l < env.obstacles.shape[0] and 0 <= y_l < env.obstacles.shape[1]) or \
                 not (0 <= x_f < env.obstacles.shape[0] and 0 <= y_f < env.obstacles.shape[1]):
                reward += REWARDS.WALL.value  # Penalty for out-of-bound situations
            elif env.obstacles[x_l, y_l] == OBSTACLE_HARD or env.obstacles[x_f, y_f] == OBSTACLE_HARD:
                reward += REWARDS.HARD_OBSTACLE.value  # Penalty for crashing into hard obstacles
            elif any(agent['position'] == new_leader_pos for agent in env.agents if agent['position'] != leader_pos) or \
                 any(agent['position'] == new_follower_pos for agent in env.agents if agent['position'] != follower_pos):
                reward += REWARDS.CRASH.value  # Penalty for crashing onto another agent
            elif distance > env.env_configurations["tetherDist"]:
                reward += REWARDS.OUT_OF_TETHER.value * tether_violated  # Penalty for being out of tether range
            total_reward += reward

            # Compute reconstruction loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(
                    np.array(leader_message[:LEADER_MESSAGE_SIZE]).reshape(1, -1),  
                    decoded_msg
                )
            )

            # Compute entropy bonus
            action_prob_leader = leader_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))

            mappo_model = MAPPO(leader_model, follower_model, encoder, decoder, lr)
            print("mappo")
            with tf.GradientTape() as tape:
                loss = mappo_model.compute_loss(
                    np.array(leader_message[:8]), decoded_msg,
                    leader_action, follower_action, reward,
                    leader_message, encoded_msg, decoded_msg
                )

            # Update Policy
            print("Update Policy")
            grads = tape.gradient(loss, leader_model.trainable_variables + follower_model.trainable_variables)
            optimizer.apply_gradients(zip(grads, leader_model.trainable_variables + follower_model.trainable_variables))
            steps_taken += 1

        avg_reward = total_reward / max_step_per_episode  # Calculate average reward
        print(f"Episode {episode + 1}: Average Reward: {avg_reward:.2f}")  # Log average reward

        # Log metrics for the episode
        episode_rewards.append(total_reward)
        episode_losses.append(float(loss))
        avg_distance = np.mean(distances) if distances else 0
        reached_goal = env.done

        # Retrieve cumulative reward from the environment's info
        cumulative_reward = env.get_info().get('cumulative_reward', 0)
        print(f"\nEpisode {episode+1} finished with Cumulative Reward: {cumulative_reward}")

        episode_logs.append({
            "episode": episode + 1,
            "reward": total_reward,
            "avg_reward": avg_reward,
            "policy_loss": float(loss),
            "contrastive_loss": float(mappo_model.compute_loss(
                state_leader=np.array(leader_message[:8]),
                decoded_msg=decoded_msg,
                action_leader=leader_action,
                action_follower=follower_action,
                reward=reward,
                leader_message=leader_message,
                encoded_message=encoded_msg,
                decoded_message=decoded_msg
            )),
            "reconstruction_loss": float(reconstruction_loss),
            "entropy": float(entropy_bonus),  # Use the initialized or computed entropy_bonus
            "success": reached_goal,
            "tether_violations": tether_violated,
            "collisions": collisions,
            "avg_distance": avg_distance,
            "hyperparams": hyperparams,
            "cumulative_reward": cumulative_reward,
            "out_of_tether_count": info['out_of_tether_count'],
            "steps_taken": steps_taken,
            "communication_count": communication_count,
        })

        if not episode_reset:
            print(f"\nEpisode {episode+1} finished with Reward: {total_reward}")
            print(f"Leader Path: {leader_path}")
            print(f"Follower Path: {follower_path}\n")

    # Export logs to a CSV file after training
    logs_df = pd.DataFrame(episode_logs)
    if not os.path.exists('logs'):
        os.mkdir('logs')
    FILENAME = "evaluation_metrics.csv"

    # Add timestamp and number of episodes to the logs
    logs_df['timestamp'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logs_df['num_episodes'] = episodes

    logs_df['algorithm'] = algorithm

    # Append to the file if it exists, otherwise create a new one
    file_path = f"logs/{FILENAME}"
    if os.path.exists(file_path):
        logs_df.to_csv(file_path, mode='a', header=False, index=False)
    else:
        logs_df.to_csv(file_path, index=False)
    
    logs_df.to_csv(f"logs/{FILENAME}", index=False)
    print(f"Training logs exported to '{FILENAME}'")


if __name__ == "main":
  env = SimpleGridEnv(
    render_mode="rgb_array", # numpy array representation
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
  )

  agents_init = [
    Agent(env, role="leader"),
    Agent(env, role="follower")
  ]
  agents = [{"id": agent._id_counter, "role": agent.role} for agent in agents_init]

  leader_pos= np.argwhere(env == LEADER)
  follower_pos= np.argwhere(env == FOLLOWER)
  target_pos = np.argwhere(env == TARGET)


  print(leader_pos)
  print(follower_pos)
  print(target_pos)

  train_MAPPO(2, leader_policy, follower_policy, encoder,leader_pos, target_pos, follower_pos, {"lr": 0.001})

  x,y = leader_pos[0]
  env[x,y]

