# -*- coding: utf-8 -*-
"""MARL_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSK3DT299xLss5u2fcOZsYO7y2oa0Pgr
"""

# # Define the Actor (Policy Network)
# class Actor(tf.keras.Model):
#     def __init__(self, num_actions):
#         super(Actor, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(num_actions, activation="softmax")

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

# # Define the Critic (Value Network)
# class Critic(tf.keras.Model):
#     def __init__(self):
#         super(Critic, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(1)

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

import tensorflow as tf
import numpy as np
import networkx as nx
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from constants import ACTION_SPACE, REWARDS
# Import the Env
import sys
import os
SIMPLEGRID_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), 'gym-simplegrid', 'gym_simplegrid', 'envs'))
sys.path.append(SIMPLEGRID_PATH)
from simple_grid import SimpleGridEnv
from agent import Agent  # Import the Agent class

FREE: int = 0
OBSTACLE_SOFT: int = 1
OBSTACLE_HARD: int = 2
AGENT: int = 3
TARGET: int = 4

# Define LEADER and FOLLOWER constants for role-based checks
LEADER = "leader"
FOLLOWER = "follower"

# Initialize the environment
env = SimpleGridEnv(
    render_mode=None,
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
)

# Modify the `new_pos` function to check roles using the Agent class
def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE, agents: list[Agent]):
    x, y = agent_position
    dx, dy = action #.value

    new_pos = (x + dx, y + dy)

    if not (0 <= new_pos[0] < env.env_configurations["rowSize"] and 0 <= new_pos[1] < env.env_configurations["colSize"]):
        return agent_position

    # Check if the new position is occupied by another agent or obstacle
    for agent in agents:
        if agent["position"] == new_pos:
            if agent.role in [LEADER, FOLLOWER]:
                return agent_position

    if env.obstacles[new_pos] in [OBSTACLE_SOFT, OBSTACLE_HARD]:
        return agent_position # Stay

    return new_pos

# Diagonal
# def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE):
#   x,y = agent_position[0]
#   dx,dy = action.value

#   new_pos = (x + dx, y+ dy)

#   if not (0 <= new_pos[0] < 10 and 0 <= new_pos[1] < 10):
#         return agent_position

#   if env[new_pos] in [LEADER, FOLLOWER, OBSTACLE_SOFT, OBSTACLE_HARD]:
#     return agent_position

#   # elif env[new_pos] == 0 or env[new_pos] == 4:
#   #   env[new_pos] == AGENT
#   #   env[agent_position] = FREE

#   return new_pos

# def calculate_reward(env, leader_pos, follower_pos, target):
#   reward = 0
#   if env[leader_pos] == target or env[follower_pos] == target:
#     reward = 50
#   elif env[leader_pos] == OBSTACLE_SOFT or env[follower_pos] == OBSTACLE_SOFT:
#     reward = -0
#   return reward



# - Suggested movement: e.g., (0,0) if leader suggests follower to stay or (0,-1) if suggested going left. Not the coordinates here!
# - Urgency level: int ranging between 1-5, the lower the more urgent.

from ast import IsNot
#window: 3X3
def get_leader_message(pos: tuple[int, int], env: SimpleGridEnv):
    """
    Generate a message from the leader agent based on its position and the environment.

    Args:
        pos (tuple[int, int]): The position of the leader agent.
        env (SimpleGridEnv): The environment instance.

    Returns:
        list: A message containing information about the environment around the leader.
        
        # Message structure :

        # - Distance to the nearest obstacle (obs_dist): int or float
        # - Relative position of the goal (xg): int, -1 if goal is not in partial observability.
        # - Relative position of the goal (yg): int, -1 if goal is not in partial observability.
        # - Whether the path is clear or blocked(path_blocked): 0/1 int
        # - Leader's action (action): int
        # - Leader can observe the follower or not (follower_visibility): bool or 0/1 int
        # - Leaders distance to follower (follower_dist): float

    Author:
        Kimia
    """
    x, y = pos  # Unpack the position
    follower_visibility = 0  # Leader cannot observe the follower initially
    follower_dist, obs_dist, counter = -1, -1, 0
    obstacles_pos, distances = [], []
    path_blocked = 0  # Path is not blocked initially
    xg, yg = (-1, -1)  # Default goal position

    for dx in range(-2, 3):
        for dy in range(-2, 3):
            nx, ny = x + dx, y + dy
            if 0 <= nx < env.env_configurations["rowSize"] and 0 <= ny < env.env_configurations["colSize"]:
                counter += 1
                # Ensure targets array has the correct dimensions
                if env.targets.shape == (env.env_configurations["rowSize"], env.env_configurations["colSize"]):
                    # Relative position to the goal
                    if env.targets[nx, ny] == env.TARGET:
                        xg, yg = nx, ny

                # Leader can see follower
                if any(agent['position'] == (nx, ny) and agent.get('role') == 'follower' for agent in env.agents):
                    follower_visibility = 1
                    follower_dist = np.sqrt((x - nx) ** 2 + (y - ny) ** 2)

                # Nearest obstacle
                if env.obstacles[nx, ny] in [env.OBSTACLE_SOFT, env.OBSTACLE_HARD]:
                    obstacles_pos.append((nx, ny))
                    dist = np.sqrt((x - nx) ** 2 + (y - ny) ** 2)
                    distances.append(dist)

    if len(distances) > 0:
        obs_dist = min(distances)
    if len(obstacles_pos) == counter:
        path_blocked = 1

    return [xg, yg, obs_dist, follower_visibility, follower_dist, path_blocked]

# LSTM
from tensorflow.keras.layers import Reshape

def build_encoder_decoder():
    input_layer = Input(shape=(8,))
    reshaped = Reshape((1, 8))(input_layer)
    x = LSTM(64, return_sequences=True)(reshaped)
    x = LSTM(32)(x)
    output_layer = Dense(8, activation="linear")(x)
    return Model(input_layer, output_layer)

encoder_decoder = build_encoder_decoder()

# MLP MAPPO
def build_policy_network():
    input_layer = Input(shape=(8,))
    x = Dense(64, activation="relu")(input_layer)
    x = Dense(64, activation="relu")(x)
    output_layer = Dense(len(ACTION_SPACE), activation="softmax")(x)
    return Model(input_layer, output_layer)

leader_policy = build_policy_network()
follower_policy = build_policy_network()

class MAPPO:
    def __init__(self, leader_model, follower_model, encoded_model, lr=0.001):
        self.leader_model = leader_model
        self.follower_model = follower_model
        self.encoded_model = encoded_model
        self.optimizer = Adam(learning_rate=lr)

    def compute_loss(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message):
        # Compute Advantage (A = R + Î³V(s') - V(s))
        value = self.leader_model(state_leader.reshape(1, -1))[0, 0]  # Predicted value
        advantage = reward - value  # TD error as Advantage Estimate
        print("loss")

        # Policy Gradient Loss (A2C)
        action_prob_leader = self.leader_model(state_leader.reshape(1, -1))
        action_prob_follower = self.follower_model(decoded_msg.reshape(1, -1))
        policy_loss = -tf.reduce_mean(advantage * tf.math.log(action_prob_leader + 1e-8))
        print('Policy Gradient Loss')
        # Contrastive Loss (CACL) for Communication Alignment
        contrastive_loss_value = contrastive_loss(tf.convert_to_tensor([encoded_message]), positive_pairs=[0])

        print('Contrastive Loss')
        # Message Reconstruction Loss (L_recon)
        print(f'leader_message={leader_message}')
        print(f'decoded_message= {decoded_message}')
        reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(leader_message, decoded_message))
        print('Entropy')
        # Entropy Bonus for Exploration
        entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))

        # Final loss function
        total_loss = policy_loss + 0.01 * entropy_bonus + 0.5 * contrastive_loss_value + 0.2 * reconstruction_loss

        return total_loss


    def apply_gradients(self, state_leader, decoded_msg, action_leader, action_follower, reward):
        with tf.GradientTape() as tape:
            loss = self.compute_loss(state_leader, decoded_msg, action_leader, action_follower, reward)
        grads = tape.gradient(loss, self.leader_model.trainable_variables + self.follower_model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.leader_model.trainable_variables + self.follower_model.trainable_variables))

# =======================
# Contrastive Learning for Communication
# =======================
def contrastive_loss(messages, positive_pairs, temperature=0.1):
    messages = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(messages)
    sim_matrix = tf.matmul(messages, messages, transpose_b=True) / temperature
    labels = tf.one_hot(positive_pairs, depth=len(messages))
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, sim_matrix)
    return loss

# Initialize the environment
env = SimpleGridEnv(
    render_mode=None,
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
)

def train_MAPPO(episodes, leader_model, follower_model, encoded_model, env):
    optimizer = Adam(learning_rate=lr)

    for episode in range(episodes):
        # Reset the environment
        obs = env.reset()
        leader_pos = env.leaders[0]['position']
        follower_pos = env.followers[0]['position']
        target_pos = np.argwhere(env.targets == env.TARGET)[0]

        total_reward = 0
        leader_path = [leader_pos]
        follower_path = [follower_pos]
        episode_reset = False

        for step in range(100):  # Limit the number of steps per episode
            # Leader generates a message and takes an action
            leader_message = get_leader_message(leader_pos)
            leader_message.append(-1)
            leader_message.append(-1)
            leader_action_probs = leader_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            leader_action = list(ACTION_SPACE)[np.argmax(leader_action_probs)]
            leader_message[6], leader_message[7] = leader_action.value
            new_leader_pos = env.step({0: leader_action.value})[1]['agent_positions'][0]

            # Encode and decode the leader's message
            encoded_msg = encoded_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            decoded_msg = encoded_msg.reshape(-1)

            # Follower takes an action based on the decoded message
            follower_action_probs = follower_model.predict(decoded_msg.reshape(1, -1))
            follower_action = list(ACTION_SPACE)[np.argmax(follower_action_probs)]
            new_follower_pos = new_pos(follower_pos, follower_action)
            print("follower")

            # Compute distance
            distance = np.sqrt((new_leader_pos[0][0] - new_follower_pos[0][0])**2 + (new_leader_pos[0][1] - new_follower_pos[0][1])**2)

            x_l, y_l = new_leader_pos[0]
            x_f, y_f = new_follower_pos[0]

            if distance > 2 or distance < 1:
                print(f"Episode {episode+1}: Distance constraint violated (Distance: {distance:.2f}). Resetting...")
                break

            elif env[x_l, y_l] == OBSTACLE_HARD or env[x_f, y_f] == OBSTACLE_HARD:
                print(f"Episode {episode+1}: Hard obstacle constraint violated. Resetting...")
                break

            # Update the path and position
            env[follower_pos] = FREE
            follower_pos = new_follower_pos
            env[follower_pos] = FOLLOWER
            follower_path.append(follower_pos.name)

            env[leader_pos] = FREE
            leader_pos = new_leader_pos
            env[leader_pos] = LEADER
            leader_path.append(leader_pos.name)

            # Compute reward
            reward -= 1
            if env[x_l, y_l] == TARGET or env[x_f, y_f] == TARGET:
                reward += 50
            elif env[x_l, y_l] == OBSTACLE_SOFT or env[x_f, y_f] == OBSTACLE_SOFT:
                reward -= 10

            total_reward += reward

            mappo_model = MAPPO(leader_model, follower_model, encoded_model)
            print("mappo")
            with tf.GradientTape() as tape:
                loss = mappo_model.compute_loss(
                    np.array(leader_message[:8]), decoded_msg,
                    leader_action, follower_action, reward,
                    leader_message, encoded_msg, decoded_msg
                )

            # Update Policy
            print("Update Policy")
            grads = tape.gradient(loss, leader_model.trainable_variables + follower_model.trainable_variables)
            optimizer.apply_gradients(zip(grads, leader_model.trainable_variables + follower_model.trainable_variables))

        if not episode_reset:
            print(f"\nEpisode {episode+1} finished with Reward: {total_reward}")
            print(f"Leader Path: {leader_path}")
            print(f"Follower Path: {follower_path}\n")


if __name__ == "main":
  env = SimpleGridEnv(
    render_mode="rgb_array", # numpy array representation
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
  )

  agents_init = [
    Agent(env, role="leader"),
    Agent(env, role="follower")
  ]
  agents = [{"id": agent._id_counter, "role": agent.role} for agent in agents_init]

  leader_pos= np.argwhere(env == LEADER)
  follower_pos= np.argwhere(env == FOLLOWER)
  target_pos = np.argwhere(env == TARGET)

  lr = 0.001

  print(leader_pos)
  print(follower_pos)
  print(target_pos)

  train_MAPPO(2, leader_policy, follower_policy, encoder_decoder,leader_pos, target_pos, follower_pos,)

  x,y = leader_pos[0]
  env[x,y]

