# -*- coding: utf-8 -*-
"""MARL_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSK3DT299xLss5u2fcOZsYO7y2oa0Pgr
"""

# # Define the Actor (Policy Network)
# class Actor(tf.keras.Model):
#     def __init__(self, num_actions):
#         super(Actor, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(num_actions, activation="softmax")

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

# # Define the Critic (Value Network)
# class Critic(tf.keras.Model):
#     def __init__(self):
#         super(Critic, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(1)

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

import tensorflow as tf
import numpy as np
from constants import ACTION_SPACE, REWARDS
# Import the Env
import sys
import os
SIMPLEGRID_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), 'gym-simplegrid', 'gym_simplegrid', 'envs'))
sys.path.append(SIMPLEGRID_PATH)
from simple_grid import SimpleGridEnv
from agent import Agent  # Import the Agent class
import pandas as pd
import datetime

FREE: int = 0
OBSTACLE_SOFT: int = 1
OBSTACLE_HARD: int = 2
AGENT: int = 3
TARGET: int = 4

# Define LEADER and FOLLOWER constants for role-based checks
LEADER = "leader"
FOLLOWER = "follower"

# Initialize the environment
env = SimpleGridEnv(
    render_mode="rgb_array", # numpy array representation
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
)

# Modify the `new_pos` function to check roles using the Agent class
def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE, agents: list[Agent]):
    x, y = agent_position
    dx, dy = action.value

    new_pos = (x + dx, y + dy)

    # Check if the new position is within the grid
    if not (0 <= new_pos[0] < env.env_configurations["rowSize"] and 0 <= new_pos[1] < env.env_configurations["colSize"]):
        print("Out of Bounds")  # Debugging message
        return agent_position  # Stay

    # Check if the new position is occupied by another agent
    for agent in agents:
        if agent["position"] == new_pos:
            print("Agent Collision")  # Debugging message
            return agent_position  # Stay

    # Check if the new position is an obstacle
    if env.obstacles[new_pos[0], new_pos[1]] in [OBSTACLE_SOFT, OBSTACLE_HARD]:
        print("Obstacle Collision")  # Debugging message
        return agent_position  # Stay

    print("Valid Move")  # Debugging message
    return new_pos

# Diagonal
# def new_pos(agent_position: tuple[int, int], action: ACTION_SPACE):
#   x,y = agent_position[0]
#   dx,dy = action.value

#   new_pos = (x + dx, y+ dy)

#   if not (0 <= new_pos[0] < 10 and 0 <= new_pos[1] < 10):
#         return agent_position

#   if env[new_pos] in [LEADER, FOLLOWER, OBSTACLE_SOFT, OBSTACLE_HARD]:
#     return agent_position

#   # elif env[new_pos] == 0 or env[new_pos] == 4:
#   #   env[new_pos] == AGENT
#   #   env[agent_position] = FREE

#   return new_pos

# def calculate_reward(env, leader_pos, follower_pos, target):
#   reward = 0
#   if env[leader_pos] == target or env[follower_pos] == target:
#     reward = 50
#   elif env[leader_pos] == OBSTACLE_SOFT or env[follower_pos] == OBSTACLE_SOFT:
#     reward = -0
#   return reward



# - Suggested movement: e.g., (0,0) if leader suggests follower to stay or (0,-1) if suggested going left. Not the coordinates here!
# - Urgency level: int ranging between 1-5, the lower the more urgent.

from ast import IsNot
#window: 3X3
def get_leader_message(pos: tuple[int, int], env: SimpleGridEnv):
    """
    Generate a message from the leader agent based on its position and the environment.

    Args:
        pos (tuple[int, int]): The position of the leader agent.
        env (SimpleGridEnv): The environment instance.

    Returns:
        list: A message containing information about the environment around the leader.
        
        # Message structure :

        # - Distance to the nearest obstacle (obs_dist): int or float
        # - Relative position of the goal (xg): int, -1 if goal is not in partial observability.
        # - Relative position of the goal (yg): int, -1 if goal is not in partial observability.
        # - Whether the path is clear or blocked(path_blocked): 0/1 int
        # - Leader's action (action): int
        # - Leader can observe the follower or not (follower_visibility): 0/1 int
        # - Leaders distance to follower (follower_dist): float

    Author:
        Kimia
    """
    x, y = pos  # Unpack the position
    follower_visibility = 0  # Leader cannot observe the follower initially
    follower_dist, obs_dist, counter = -1, -1, 0
    obstacles_pos, distances = [], []
    path_blocked = 0  # Path is not blocked initially
    xg, yg = (-1, -1)  # Default goal position

    for dx in range(-2, 3):
        for dy in range(-2, 3):
            nx, ny = x + dx, y + dy
            if 0 <= nx < env.env_configurations["rowSize"] and 0 <= ny < env.env_configurations["colSize"]:
                counter += 1
                # Ensure targets array has the correct dimensions
                if env.targets.shape == (env.env_configurations["rowSize"], env.env_configurations["colSize"]):
                    # Relative position to the goal
                    if env.targets[nx, ny] == env.TARGET:
                        xg, yg = nx, ny

                # Leader can see follower
                if any(agent['position'] == (nx, ny) and agent.get('role') == 'follower' for agent in env.agents):
                    follower_visibility = 1
                    follower_dist = np.sqrt((x - nx) ** 2 + (y - ny) ** 2)

                # Nearest obstacle
                if env.obstacles[nx, ny] in [env.OBSTACLE_SOFT, env.OBSTACLE_HARD]:
                    obstacles_pos.append((nx, ny))
                    dist = np.sqrt((x - nx) ** 2 + (y - ny) ** 2)
                    distances.append(dist)

    if len(distances) > 0:
        obs_dist = min(distances)
    if len(obstacles_pos) == counter:
        path_blocked = 1

    return [xg, yg, obs_dist, follower_visibility, follower_dist, path_blocked]

# LSTM
def build_encoder_decoder():
    input_layer = tf.keras.layers.Input(shape=(8,))
    reshaped = tf.keras.layers.Reshape((1, 8))(input_layer)
    x = tf.keras.layers.LSTM(64, return_sequences=True)(reshaped)
    x = tf.keras.layers.LSTM(32)(x)
    output_layer = tf.keras.layers.Dense(8, activation="linear")(x)
    return tf.keras.models.Model(input_layer, output_layer)

encoder_decoder = build_encoder_decoder()

# MLP MAPPO
def build_policy_network():
    input_layer = tf.keras.layers.Input(shape=(8,))
    # hidden layers
    x = tf.keras.layers.Dense(64, activation="relu")(input_layer)
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    # output layer
    output_layer = tf.keras.layers.Dense(len(ACTION_SPACE), activation="softmax")(x)
    return tf.keras.models.Model(input_layer, output_layer)

leader_policy = build_policy_network()
follower_policy = build_policy_network()

class MAPPO:
    def __init__(self, leader_model, follower_model, encoded_model, lr=0.001):
        self.leader_model = leader_model
        self.follower_model = follower_model
        self.encoded_model = encoded_model
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    def compute_loss(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message, hyperparams: dict = None):
        # Hyperparameters
        contrastive_weight = 0.5  # Default value
        reconstruction_loss_weight = 0.2  # Default value
        entropy_bonus_weight = 0.01  # Default value
        if hyperparams:
            contrastive_weight = hyperparams.get('contrastive_weight', contrastive_weight)
            reconstruction_loss_weight = hyperparams.get('reconstruction_loss_weight', reconstruction_loss_weight)
            entropy_bonus_weight = hyperparams.get('entropy_bonus_weight', entropy_bonus_weight)
        # Convert leader_message and decoded_message to NumPy arrays
        leader_message = np.array(leader_message)
        decoded_message = np.array(decoded_message)

        # Compute Advantage (A = R + Î³V(s') - V(s))
        value = self.leader_model(state_leader.reshape(1, -1))[0, 0]  # Predicted value
        advantage = reward - value  # TD error as Advantage Estimate
        print("loss", advantage)

        # Policy Gradient Loss (A2C)
        action_prob_leader = self.leader_model(state_leader.reshape(1, -1))
        action_prob_follower = self.follower_model(decoded_msg.reshape(1, -1))
        policy_loss = -tf.reduce_mean(advantage * tf.math.log(action_prob_leader + 1e-8))
        print('Policy Gradient Loss', policy_loss)
        
        # Contrastive Loss (CACL) for Communication Alignment
        contrastive_loss_value = contrastive_loss(tf.convert_to_tensor([encoded_message]), positive_pairs=[0])
        print('Contrastive Loss', contrastive_loss_value)

        # Message Reconstruction Loss (L_recon)
        print(f'leader_message={leader_message}')
        print(f'decoded_message= {decoded_message}')

        # Align shapes of leader_message and decoded_message
        min_dim = min(leader_message.shape[-1], decoded_message.shape[-1])
        leader_message_aligned = leader_message[..., :min_dim]
        decoded_message_aligned = decoded_message[..., :min_dim]

        reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(leader_message_aligned, decoded_message_aligned))
        print('Reconstruction Loss', reconstruction_loss)

        # Entropy Bonus for Exploration
        entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))
        print('Entropy Bonus', entropy_bonus)

        # Final loss function
        total_loss = policy_loss + entropy_bonus_weight * entropy_bonus + contrastive_weight * contrastive_loss_value + reconstruction_loss_weight * reconstruction_loss
        print('Total Loss', total_loss)

        return total_loss


    def apply_gradients(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message):
        with tf.GradientTape() as tape:
            loss = self.compute_loss(
                state_leader=state_leader,
                decoded_msg=decoded_msg,
                action_leader=action_leader,
                action_follower=action_follower,
                reward=reward,
                leader_message=leader_message,
                encoded_message=encoded_message,
                decoded_message=decoded_message
            )
        grads = tape.gradient(loss, self.leader_model.trainable_variables + self.follower_model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.leader_model.trainable_variables + self.follower_model.trainable_variables))

# =======================
# Contrastive Learning for Communication
# =======================
def contrastive_loss(messages, positive_pairs, temperature=0.1):
    messages = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(messages)
    sim_matrix = tf.matmul(messages, messages, transpose_b=True) / temperature
    labels = tf.one_hot(positive_pairs, depth=len(messages))
    loss = tf.keras.losses.binary_crossentropy(from_logits=True)(labels, sim_matrix)
    return loss

def train_MAPPO(episodes, leader_model, follower_model, encoded_model, env, hyperparams: dict = None):
    print("Starting training...")
    # Logging
    episode_rewards = []
    episode_losses = []
    episode_logs = []  # To store detailed logs for each episode

    # Hyperparameters
    lr = 0.001  # Default learning rate
    max_step_per_episode = 100  # Default max steps per episode
    max_episodes = 100  # Default max episodes
    if hyperparams:
        lr = hyperparams.get('lr', lr)
        max_step_per_episode = hyperparams.get('max_steps', max_step_per_episode)
        max_episodes = hyperparams.get('max_episodes', max_episodes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    total_rewards = []
    success_rate = 0
    collision_count = 0

    # Initialize MAPPO model
    mappo_model = MAPPO(leader_model, follower_model, encoded_model, lr)

    episodes = episodes if (episodes is not None or episodes > 0) else max_episodes
    for episode in range(episodes):
        print(f"\nEpisode {episode + 1}/{episodes}")
        # Reset the environment
        obs = env.reset()
        leader_pos = env.leaders[0]['position']
        follower_pos = env.followers[0]['position']

        # Ensure there are targets in the environment
        target_positions = np.argwhere(env.targets == env.TARGET)
        target_pos = target_positions[0] if len(target_positions) > 0 else None
        
        episode_reset = False
        total_reward = 0
        leader_path = [leader_pos]
        follower_path = [follower_pos]

        reward = 0
        tether_violated = 0
        collisions = 0
        distances = []

        reconstruction_loss = 0  # Initialize reconstruction_loss to avoid UnboundLocalError
        entropy_bonus = 0  # Initialize entropy_bonus to avoid UnboundLocalError
        loss = 0  # Initialize loss to avoid UnboundLocalError
        for step in range(max_step_per_episode):  # Limit the number of steps per episode
            print(f"Step {step + 1}/{max_step_per_episode}")
            # Leader generates a message and takes an action
            leader_message = get_leader_message(leader_pos, env)
            leader_message.append(-1)
            leader_message.append(-1)
            leader_action_probs = leader_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            leader_action = list(ACTION_SPACE)[np.argmax(leader_action_probs)]
            leader_message[6], leader_message[7] = leader_action.value

            # Update leader position using the step method
            _, _, _, _, info = env.step({0: leader_action.value})
            new_leader_pos = info['agent_positions'][0]

            # Encode and decode the leader's message
            encoded_msg = encoded_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            decoded_msg = encoded_msg.reshape(-1)

            # Follower takes an action based on the decoded message
            follower_action_probs = follower_model.predict(decoded_msg.reshape(1, -1))
            follower_action = list(ACTION_SPACE)[np.argmax(follower_action_probs)]
            new_follower_pos = new_pos(follower_pos, follower_action, env.agents)  # Pass the agents list
            print("follower")

            # Compute distance
            distance = np.sqrt((new_leader_pos[0] - new_follower_pos[0])**2 + (new_leader_pos[1] - new_follower_pos[1])**2)
            distances.append(distance)

            x_l, y_l = new_leader_pos
            x_f, y_f = new_follower_pos

            # Use tetherDist from the environment configuration
            tether_limit = env.env_configurations["tetherDist"]
            if distance > tether_limit or distance < 1:
                tether_violated += 1
                print(f"Episode {episode+1}: Tether constraint violated (Distance: {distance:.2f}, Tether Limit: {tether_limit}). Resetting...")
                break

            elif env.obstacles[x_l, y_l] == OBSTACLE_HARD or env.obstacles[x_f, y_f] == OBSTACLE_HARD:
                collisions += 1
                print(f"Episode {episode+1}: Hard obstacle constraint violated. Resetting...")
                break

            # Update the path and position
            for agent in env.agents:
                if agent['position'] == follower_pos:
                    agent['position'] = new_follower_pos
                    break
            follower_pos = new_follower_pos
            follower_path.append(follower_pos)

            for agent in env.agents:
                if agent['position'] == leader_pos:
                    agent['position'] = new_leader_pos
                    break
            leader_pos = new_leader_pos
            leader_path.append(leader_pos)

            # Compute reward
            reward -= 1
            if (0 <= x_l < env.targets.shape[0] and 0 <= y_l < env.targets.shape[1] and env.targets[x_l, y_l] == TARGET) or \
               (0 <= x_f < env.targets.shape[0] and 0 <= y_f < env.targets.shape[1] and env.targets[x_f, y_f] == TARGET):
                reward += REWARDS.TARGET.value
            elif (0 <= x_l < env.obstacles.shape[0] and 0 <= y_l < env.obstacles.shape[1] and env.obstacles[x_l, y_l] == OBSTACLE_SOFT) or \
                 (0 <= x_f < env.obstacles.shape[0] and 0 <= y_f < env.obstacles.shape[1] and env.obstacles[x_f, y_f] == OBSTACLE_SOFT):
                reward += REWARDS.SOFT_OBSTACLE.value
            elif not (0 <= x_l < env.obstacles.shape[0] and 0 <= y_l < env.obstacles.shape[1]) or \
                 not (0 <= x_f < env.obstacles.shape[0] and 0 <= y_f < env.obstacles.shape[1]):
                reward += REWARDS.WALL.value  # Penalty for out-of-bound situations
            elif env.obstacles[x_l, y_l] == OBSTACLE_HARD or env.obstacles[x_f, y_f] == OBSTACLE_HARD:
                reward += REWARDS.HARD_OBSTACLE.value  # Penalty for crashing into hard obstacles
            elif any(agent['position'] == new_leader_pos for agent in env.agents if agent['position'] != leader_pos) or \
                 any(agent['position'] == new_follower_pos for agent in env.agents if agent['position'] != follower_pos):
                reward += REWARDS.CRASH.value  # Penalty for crashing onto another agent

            total_reward += reward

            # Compute reconstruction loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(
                    np.array(leader_message[:8]), 
                    decoded_msg
                )
            )

            # Compute entropy bonus
            action_prob_leader = leader_model.predict(np.array(leader_message[:8]).reshape(1, -1))
            entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))

            mappo_model = MAPPO(leader_model, follower_model, encoded_model)
            print("mappo")
            with tf.GradientTape() as tape:
                loss = mappo_model.compute_loss(
                    np.array(leader_message[:8]), decoded_msg,
                    leader_action, follower_action, reward,
                    leader_message, encoded_msg, decoded_msg
                )

            # Update Policy
            print("Update Policy")
            grads = tape.gradient(loss, leader_model.trainable_variables + follower_model.trainable_variables)
            optimizer.apply_gradients(zip(grads, leader_model.trainable_variables + follower_model.trainable_variables))

        avg_reward = total_reward / max_step_per_episode  # Calculate average reward
        print(f"Episode {episode + 1}: Average Reward: {avg_reward:.2f}")  # Log average reward

        # Log metrics for the episode
        episode_rewards.append(total_reward)
        episode_losses.append(float(loss))
        avg_distance = np.mean(distances) if distances else 0
        reached_goal = env.done

        # Retrieve cumulative reward from the environment's info
        cumulative_reward = env.get_info().get('cumulative_reward', 0)
        print(f"\nEpisode {episode+1} finished with Cumulative Reward: {cumulative_reward}")

        episode_logs.append({
            "episode": episode + 1,
            "reward": total_reward,
            "avg_reward": avg_reward,
            "policy_loss": float(loss),
            "contrastive_loss": float(mappo_model.compute_loss(
                state_leader=np.array(leader_message[:8]),
                decoded_msg=decoded_msg,
                action_leader=leader_action,
                action_follower=follower_action,
                reward=reward,
                leader_message=leader_message,
                encoded_message=encoded_msg,
                decoded_message=decoded_msg
            )),
            "reconstruction_loss": float(reconstruction_loss),
            "entropy": float(entropy_bonus),  # Use the initialized or computed entropy_bonus
            "success": reached_goal,
            "tether_violations": tether_violated,
            "collisions": collisions,
            "avg_distance": avg_distance,
            "hyperparams": hyperparams,
            "cumulative_reward": cumulative_reward
        })

        if not episode_reset:
            print(f"\nEpisode {episode+1} finished with Reward: {total_reward}")
            print(f"Leader Path: {leader_path}")
            print(f"Follower Path: {follower_path}\n")

    # Export logs to a CSV file after training
    logs_df = pd.DataFrame(episode_logs)
    if not os.path.exists('logs'):
        os.mkdir('logs')
    FILENAME = "evaluation_metrics.csv"

    # Add timestamp and number of episodes to the logs
    logs_df['timestamp'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logs_df['num_episodes'] = episodes

    # Append to the file if it exists, otherwise create a new one
    file_path = f"logs/{FILENAME}"
    if os.path.exists(file_path):
        logs_df.to_csv(file_path, mode='a', header=False, index=False)
    else:
        logs_df.to_csv(file_path, index=False)
    
    logs_df.to_csv(f"logs/{FILENAME}", index=False)
    print(f"Training logs exported to '{FILENAME}'")


if __name__ == "main":
  env = SimpleGridEnv(
    render_mode="rgb_array", # numpy array representation
    rowSize=10,
    colSize=10,
    num_soft_obstacles=10,
    num_hard_obstacles=5,
    num_robots=2,
    tetherDist=2,
    num_leaders=1,
    num_target=1
  )

  agents_init = [
    Agent(env, role="leader"),
    Agent(env, role="follower")
  ]
  agents = [{"id": agent._id_counter, "role": agent.role} for agent in agents_init]

  leader_pos= np.argwhere(env == LEADER)
  follower_pos= np.argwhere(env == FOLLOWER)
  target_pos = np.argwhere(env == TARGET)


  print(leader_pos)
  print(follower_pos)
  print(target_pos)

  train_MAPPO(2, leader_policy, follower_policy, encoder_decoder,leader_pos, target_pos, follower_pos, {"lr": 0.001})

  x,y = leader_pos[0]
  env[x,y]

