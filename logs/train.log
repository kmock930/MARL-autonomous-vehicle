2025-03-30 16:58:50.203136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-30 16:58:50.222969: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-30 16:58:50.229225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-30 16:58:50.245298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1743368333.027895 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.112609 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.113229 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.119260 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.119756 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.120147 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.399007 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.399514 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1743368333.399947 2612722 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-03-30 16:58:53.400267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6799 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:00:05.0, compute capability: 7.5
2025-03-30 16:58:54.908965: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.
2025-03-30 16:58:54.909080: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.
2025-03-30 16:58:54.909190: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:892] Profiler found 1 GPUs
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
I0000 00:00:1743368335.518985 2612816 service.cc:146] XLA service 0x7f40800e81e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1743368335.519149 2612816 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 SUPER, Compute Capability 7.5
2025-03-30 16:58:55.533935: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-03-30 16:58:55.566946: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90101
I0000 00:00:1743368335.746069 2612816 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Training with parameters: {'lr': 0.006435, 'episodes': 50, 'contrastive_weight': 0.74, 'reconstruction_weight': 0.3, 'entropy_weight': 0.043, 'max_steps': 200}
Starting training...

Episode 1/50
Starting reset process
Data: [[0 0 0 1 0 0 1 1 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [4 0 0 0 2 0 0 0 1 0]
 [0 1 1 0 0 0 0 0 2 0]
 [0 0 2 0 0 0 1 0 0 0]
 [0 0 0 3 0 3 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 3), 'role': 'leader'}, {'position': (7, 5), 'role': 'follower'}]
Data: [[0 0 0 1 0 0 1 1 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [4 0 0 0 2 0 0 0 1 0]
 [0 1 1 0 0 0 0 0 2 0]
 [0 0 2 0 0 0 1 0 0 0]
 [0 0 0 3 0 3 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 3), 'role': 'leader'}, {'position': (7, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 312ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 313ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 326ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 326ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 169ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 169ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
/home/student/miniconda3/envs/marl_env/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?
  warnings.warn(
mappo
loss tf.Tensor(-1.1405872, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2.5512352, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
  0.          1.        ]
decoded_message= [ 0.00902222  0.0212246   0.00404351  0.01455873 -0.00809918 -0.01346759
 -0.01730091 -0.01285219]
Reconstruction Loss tf.Tensor(1.259939, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.24022453, shape=(), dtype=float32)
Total Loss tf.Tensor(-2.2968452, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-2.2567072, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-5.1894026, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.23189074, shape=(), dtype=float32)
Total Loss tf.Tensor(-4.912394, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-3.4180374, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-8.555923, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.20482567, shape=(), dtype=float32)
Total Loss tf.Tensor(-8.30419, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 0 0 0 0 0 1 0 0 0]
 [2 0 2 0 0 0 0 0 0 1]
 [2 0 0 0 0 0 0 0 0 0]
 [1 0 0 4 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [1 1 0 0 0 0 3 0 3 0]
 [0 2 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 1 0 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 2 0 0 0 0 0]]
Agents: [{'position': (5, 6), 'role': 'leader'}, {'position': (5, 8), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
Episode 1: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 1: Average Reward: -0.03

Episode 1 finished with Cumulative Reward: 0
loss tf.Tensor(-3.5141325, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-9.461317, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.18128143, shape=(), dtype=float32)
Total Loss tf.Tensor(-9.284929, shape=(), dtype=float32)

Episode 1 finished with Reward: -6
Leader Path: [(7, 3), (7, 4), (6, 4), (5, 4)]
Follower Path: [(7, 5), (6, 5), (5, 5), (4, 5)]


Episode 2/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 1 2 0 0]
 [0 0 0 0 1 1 0 0 2 0]
 [0 0 0 0 0 0 1 0 2 0]
 [0 0 0 0 4 0 1 0 0 2]
 [0 0 0 0 0 0 3 0 0 0]
 [0 0 1 0 0 0 0 0 0 1]
 [0 0 0 1 0 0 3 0 0 0]
 [0 0 1 0 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (5, 6), 'role': 'leader'}, {'position': (7, 6), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-1.591723, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-4.566329, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01026403  0.02150278  0.00352939  0.01948205 -0.01015675 -0.01625296
 -0.02962391 -0.01658937]
Reconstruction Loss tf.Tensor(1.1251712, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.16093148, shape=(), dtype=float32)
Total Loss tf.Tensor(-4.3396854, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-2.6900487, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-8.453903, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.13203001, shape=(), dtype=float32)
Total Loss tf.Tensor(-8.278008, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-3.7859945, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-13.3530035, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.09953357, shape=(), dtype=float32)
Total Loss tf.Tensor(-13.202632, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 55ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 55ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
Valid Move
follower
Episode 2: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 2: Average Reward: -0.03

Episode 2 finished with Cumulative Reward: -40
loss tf.Tensor(-3.9249554, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-18.28515, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.042402938, shape=(), dtype=float32)
Total Loss tf.Tensor(-18.110151, shape=(), dtype=float32)

Episode 2 finished with Reward: -6
Leader Path: [(5, 6), (5, 6), (5, 6), (5, 6)]
Follower Path: [(7, 6), (6, 5), (5, 5), (4, 5)]


Episode 3/50
Starting reset process
Data: [[0 0 2 0 0 0 0 3 0 0]
 [0 0 0 2 0 3 0 0 0 1]
 [1 0 1 0 0 0 0 4 0 1]
 [0 0 1 2 0 1 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 2 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (0, 7), 'role': 'leader'}, {'position': (1, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of bounds
Starting reset process
Data: [[0 0 2 0 0 0 0 0 0 1]
 [0 0 0 0 0 1 0 0 3 0]
 [0 0 0 0 1 0 2 0 3 1]
 [1 0 0 0 0 2 0 0 0 0]
 [0 0 0 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 1 0 0 0 0 0 2 0 0]
 [1 0 0 0 0 0 1 0 1 0]
 [0 0 0 0 0 0 0 4 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (1, 8), 'role': 'leader'}, {'position': (2, 8), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 3: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 3: Average Reward: 0.00

Episode 3 finished with Cumulative Reward: 0
loss tf.Tensor(-0.98080945, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-5.9809256, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01692285  0.02223727  0.0072925   0.02150954 -0.01300852 -0.0026801
 -0.03053134 -0.01128961]
Reconstruction Loss tf.Tensor(1.7502918, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.013471333, shape=(), dtype=float32)
Total Loss tf.Tensor(-5.630732, shape=(), dtype=float32)

Episode 3 finished with Reward: 0
Leader Path: [(0, 7)]
Follower Path: [(1, 5)]


Episode 4/50
Starting reset process
Data: [[0 3 0 0 2 0 0 2 1 0]
 [0 0 1 0 0 0 0 0 0 0]
 [3 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 2]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1]
 [0 0 0 0 0 2 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 4]]
Agents: [{'position': (2, 0), 'role': 'leader'}, {'position': (0, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
Out of Bounds
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-1.9620823, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-10.740223, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.023562279, shape=(), dtype=float32)
Total Loss tf.Tensor(-10.489707, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of Bounds
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-2.979089, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-17.939796, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.014390349, shape=(), dtype=float32)
Total Loss tf.Tensor(-17.740017, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Out of bounds
Starting reset process
Data: [[0 0 0 0 1 0 0 0 1 0]
 [0 0 0 0 2 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 2]
 [2 0 0 3 3 4 0 0 0 1]
 [1 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 1 2 0 0 0 0]
 [0 1 0 0 0 0 2 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]]
Agents: [{'position': (3, 4), 'role': 'leader'}, {'position': (3, 3), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
Episode 4: Tether constraint violated (Distance: 4.24, Tether Limit: 2). Resetting...
Episode 4: Average Reward: -0.01

Episode 4 finished with Cumulative Reward: 0
loss tf.Tensor(-2.9952004, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-22.658031, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.0080243   0.01689131 -0.00063591  0.01811047 -0.00769391  0.01200914
 -0.02407573 -0.00583469]
Reconstruction Loss tf.Tensor(1.2481482, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.0040162923, shape=(), dtype=float32)
Total Loss tf.Tensor(-22.408361, shape=(), dtype=float32)

Episode 4 finished with Reward: -3
Leader Path: [(2, 0), (1, 0), (0, 0)]
Follower Path: [(0, 1), (0, 1), (0, 1)]


Episode 5/50
Starting reset process
Data: [[0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 2 0]
 [0 0 0 4 1 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 1]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 2 1 0 0 1]
 [0 0 0 0 0 2 0 0 0 0]
 [1 0 0 0 0 3 2 2 0 0]
 [0 0 0 0 0 0 0 3 0 1]]
Agents: [{'position': (8, 5), 'role': 'leader'}, {'position': (9, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 2 0 0 0 0 0 0 0 0]
 [2 0 0 1 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 4 0 0 0 0 0 2 0 1]
 [1 0 2 0 0 0 0 0 0 0]
 [0 3 0 0 1 0 0 0 0 0]
 [3 0 0 1 0 0 0 1 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 1 0]]
Agents: [{'position': (7, 0), 'role': 'leader'}, {'position': (6, 1), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 5: Tether constraint violated (Distance: 6.08, Tether Limit: 2). Resetting...
Episode 5: Average Reward: 0.00

Episode 5 finished with Cumulative Reward: 0
loss tf.Tensor(-0.9968477, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-8.255737, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.0026849895, shape=(), dtype=float32)
Total Loss tf.Tensor(-8.00543, shape=(), dtype=float32)

Episode 5 finished with Reward: 0
Leader Path: [(8, 5)]
Follower Path: [(9, 7)]


Episode 6/50
Starting reset process
Data: [[0 0 0 0 2 1 0 0 4 0]
 [0 0 2 0 1 0 0 0 0 1]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 2 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 3 0 3 0 0 0]
 [1 0 0 0 0 2 1 0 0 0]
 [0 1 1 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 4), 'role': 'leader'}, {'position': (7, 6), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-1.9969748, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.472078, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.0026096331, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.222034, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-2.99938, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-30.124264, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.0006292312, shape=(), dtype=float32)
Total Loss tf.Tensor(-29.87424, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-3.9998388, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-46.345512, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01026403  0.02150278  0.00352939  0.01948205 -0.01015675 -0.01625296
 -0.02962391 -0.01658937]
Reconstruction Loss tf.Tensor(1.1251712, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.00018465953, shape=(), dtype=float32)
Total Loss tf.Tensor(-46.12048, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-4.9998226, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-56.299404, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.00020359214, shape=(), dtype=float32)
Total Loss tf.Tensor(-56.150024, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 6: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 6: Average Reward: -0.05

Episode 6 finished with Cumulative Reward: -23
loss tf.Tensor(-4.9999905, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-70.64463, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.4007769e-05, shape=(), dtype=float32)
Total Loss tf.Tensor(-70.470055, shape=(), dtype=float32)

Episode 6 finished with Reward: -10
Leader Path: [(7, 4), (6, 4), (5, 4), (4, 4), (4, 4)]
Follower Path: [(7, 6), (6, 6), (5, 6), (4, 5), (3, 5)]


Episode 7/50
Starting reset process
Data: [[0 0 0 1 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 2 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 1 0 1 0 1]
 [1 0 0 4 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 1 0 2 0 2 1 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 3 3]]
Agents: [{'position': (9, 8), 'role': 'leader'}, {'position': (9, 9), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-1.9994059, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-20.29975, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1. -1.  1.  1.  0. -1.  0.]
decoded_message= [-0.00483244  0.01431952 -0.0047345   0.00956707 -0.00266428 -0.03045193
 -0.0235924  -0.02220621]
Reconstruction Loss tf.Tensor(0.74385524, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(0.00059896364, shape=(), dtype=float32)
Total Loss tf.Tensor(-20.150972, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-48.188408, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.9646932e-07, shape=(), dtype=float32)
Total Loss tf.Tensor(-47.91372, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-64.71706, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.740449e-08, shape=(), dtype=float32)
Total Loss tf.Tensor(-64.51742, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.533, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.6137696e-08, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.35842, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-16.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-261.87912, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.2263681e-09, shape=(), dtype=float32)
Total Loss tf.Tensor(-261.70456, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-17.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-278.3546, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.5444737e-11, shape=(), dtype=float32)
Total Loss tf.Tensor(-278.15497, shape=(), dtype=float32)
Update Policy
Step 7/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-28.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-458.46924, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.8191564e-12, shape=(), dtype=float32)
Total Loss tf.Tensor(-458.29468, shape=(), dtype=float32)
Update Policy
Step 8/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-29.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-474.84424, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.2773537e-13, shape=(), dtype=float32)
Total Loss tf.Tensor(-474.6446, shape=(), dtype=float32)
Update Policy
Step 9/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-30.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-491.21814, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.195692e-14, shape=(), dtype=float32)
Total Loss tf.Tensor(-491.04358, shape=(), dtype=float32)
Update Policy
Step 10/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Out of bounds
Starting reset process
Data: [[0 2 0 0 0 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 1 4 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 1 0 0 0 0]
 [0 1 1 0 1 0 3 2 3 2]
 [0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 2 1 0 0 0 0]]
Agents: [{'position': (6, 8), 'role': 'leader'}, {'position': (6, 6), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
Episode 7: Tether constraint violated (Distance: 6.08, Tether Limit: 2). Resetting...
Episode 7: Average Reward: -0.62

Episode 7 finished with Cumulative Reward: 0
loss tf.Tensor(-30.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-491.21814, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.51295e-16, shape=(), dtype=float32)
Total Loss tf.Tensor(-491.0185, shape=(), dtype=float32)

Episode 7 finished with Reward: -125
Leader Path: [(9, 8), (8, 8), (7, 8), (6, 8), (5, 8), (4, 8), (3, 8), (2, 8), (1, 8), (0, 8)]
Follower Path: [(9, 9), (9, 9), (8, 9), (7, 9), (6, 9), (5, 9), (4, 9), (3, 9), (2, 9), (1, 9)]


Episode 8/50
Starting reset process
Data: [[0 0 0 0 1 0 0 0 0 3]
 [1 0 0 0 0 0 0 3 1 0]
 [1 0 0 0 0 0 2 0 0 0]
 [2 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 1]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 1 0]
 [0 0 0 2 0 2 4 1 0 0]]
Agents: [{'position': (1, 7), 'role': 'leader'}, {'position': (0, 9), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
Out of Bounds
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.7332297e-17, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497597, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Out of bounds
Starting reset process
Data: [[0 0 0 0 0 0 1 1 0 0]
 [0 1 0 0 0 2 1 0 0 0]
 [0 0 0 0 0 0 0 0 2 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 2 0 0 0 0 1 0 0]
 [0 0 0 0 2 0 1 0 0 0]
 [0 0 1 0 4 0 0 2 0 0]
 [0 0 0 0 0 0 0 0 0 3]
 [0 1 0 0 0 0 0 3 0 0]
 [0 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 9), 'role': 'leader'}, {'position': (8, 7), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of Bounds
follower
Episode 8: Tether constraint violated (Distance: 7.00, Tether Limit: 2). Resetting...
Episode 8: Average Reward: -0.01

Episode 8 finished with Cumulative Reward: 0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.8258473e-18, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.49786, shape=(), dtype=float32)

Episode 8 finished with Reward: -1
Leader Path: [(1, 7), (0, 7)]
Follower Path: [(0, 9), (0, 9)]


Episode 9/50
Starting reset process
Data: [[4 0 0 0 0 0 0 0 0 0]
 [0 3 0 1 0 1 0 0 2 0]
 [1 1 0 0 0 2 0 0 1 0]
 [1 0 0 3 2 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 0 2 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 1 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (1, 1), 'role': 'leader'}, {'position': (3, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Out of tethered distance
Starting reset process
Data: [[0 0 0 0 0 0 0 1 0 0]
 [2 0 1 2 0 0 0 0 0 3]
 [1 0 1 1 0 1 0 0 0 3]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 2 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 2 2 0 0]
 [0 0 0 0 0 0 4 0 0 0]]
Agents: [{'position': (1, 9), 'role': 'leader'}, {'position': (2, 9), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
Episode 9: Tether constraint violated (Distance: 7.07, Tether Limit: 2). Resetting...
Episode 9: Average Reward: 0.00

Episode 9 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.82842712  0.
 -1.          0.        ]
decoded_message= [ 0.01686088  0.02525865  0.00916713  0.02193517 -0.01472717 -0.02680259
 -0.03339709 -0.01950858]
Reconstruction Loss tf.Tensor(1.6303084, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.0670642e-20, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.047878, shape=(), dtype=float32)

Episode 9 finished with Reward: 0
Leader Path: [(1, 1)]
Follower Path: [(3, 3)]


Episode 10/50
Starting reset process
Data: [[0 0 4 0 1 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 2 0]
 [0 1 1 0 0 0 0 0 0 1]
 [0 1 0 3 0 0 2 0 0 1]
 [0 3 0 0 0 0 0 0 0 1]
 [0 0 2 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 2 0]]
Agents: [{'position': (6, 3), 'role': 'leader'}, {'position': (7, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 10: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 10: Average Reward: 0.00

Episode 10 finished with Cumulative Reward: 50
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01369481  0.0226134   0.00579102  0.02079432 -0.01190857 -0.01366198
 -0.03071866 -0.01548924]
Reconstruction Loss tf.Tensor(1.376143, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.678224e-19, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.098711, shape=(), dtype=float32)

Episode 10 finished with Reward: 0
Leader Path: [(6, 3)]
Follower Path: [(7, 1)]


Episode 11/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 1 0]
 [0 1 0 0 0 0 0 0 0 1]
 [2 0 0 0 0 0 0 0 1 0]
 [0 0 0 3 0 0 0 1 0 2]
 [0 0 0 3 0 0 0 0 0 0]
 [0 0 1 2 0 0 0 0 0 1]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 4 1 0 0 0 0]
 [0 0 2 0 0 0 0 0 2 0]]
Agents: [{'position': (3, 3), 'role': 'leader'}, {'position': (4, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00689538  0.01692231 -0.00111776  0.01790977 -0.00714325  0.00902419
 -0.02400513 -0.0072077 ]
Reconstruction Loss tf.Tensor(1.1229975, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.2437797e-16, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.52328, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Out of tethered distance
Starting reset process
Data: [[0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 1 0 1 0 0 2 0 0 1]
 [1 0 0 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 2]
 [0 0 2 0 0 0 0 0 0 0]
 [0 0 3 4 2 0 0 0 1 0]
 [0 0 3 0 0 0 1 0 0 0]
 [0 0 0 1 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 2), 'role': 'leader'}, {'position': (6, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
/home/student/Public/MARL-autonomous-vehicle/gym-simplegrid/gym_simplegrid/envs/simple_grid.py:676: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots(tight_layout=True)
Valid Move
follower
Episode 11: Tether constraint violated (Distance: 4.12, Tether Limit: 2). Resetting...
Episode 11: Average Reward: -0.01

Episode 11 finished with Cumulative Reward: 0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.815936e-21, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)

Episode 11 finished with Reward: -1
Leader Path: [(3, 3), (2, 3)]
Follower Path: [(4, 3), (4, 3)]


Episode 12/50
Starting reset process
Data: [[1 0 0 0 0 0 0 0 3 0]
 [0 0 0 0 0 1 0 0 0 3]
 [0 0 4 0 0 0 0 0 0 0]
 [0 0 1 1 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 1 0 0 1]
 [0 0 0 0 1 0 2 0 2 1]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 2 0 2 0]]
Agents: [{'position': (0, 8), 'role': 'leader'}, {'position': (1, 9), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Out of bounds
Starting reset process
Data: [[0 1 0 0 0 2 0 0 0 0]
 [3 0 0 0 0 0 0 0 1 0]
 [1 2 0 1 0 0 0 0 0 0]
 [0 3 2 0 0 1 1 0 0 0]
 [0 0 2 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 2 0 1 0 0 0]
 [4 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]]
Agents: [{'position': (3, 1), 'role': 'leader'}, {'position': (1, 0), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 12: Tether constraint violated (Distance: 7.62, Tether Limit: 2). Resetting...
Episode 12: Average Reward: 0.00

Episode 12 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.373936, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.         -1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [-0.00141511  0.0165085  -0.00228688  0.01135618 -0.00518181 -0.03547552
 -0.02634881 -0.02336169]
Reconstruction Loss tf.Tensor(0.8709742, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.7686635e-13, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.199741, shape=(), dtype=float32)

Episode 12 finished with Reward: 0
Leader Path: [(0, 8)]
Follower Path: [(1, 9)]


Episode 13/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 1 0 0]
 [0 0 0 3 0 0 0 0 1 0]
 [1 1 0 0 0 0 2 1 0 0]
 [0 1 2 3 0 0 4 0 0 0]
 [0 0 0 0 0 2 0 0 2 0]
 [0 0 0 0 0 0 1 0 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 1]]
Agents: [{'position': (3, 3), 'role': 'leader'}, {'position': (5, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Out of tethered distance
Starting reset process
Data: [[0 1 0 1 0 0 0 2 1 0]
 [0 4 0 0 0 0 0 0 0 0]
 [0 0 0 2 1 0 0 0 0 0]
 [0 0 0 3 3 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 1 2]
 [0 1 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 1 1 0 0 0 0 0 0]]
Agents: [{'position': (3, 3), 'role': 'leader'}, {'position': (3, 4), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.815936e-21, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 0 2 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 1 2 0]
 [0 0 0 2 0 0 0 1 0 0]
 [2 0 0 0 0 0 2 0 0 1]
 [0 0 0 0 3 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 3 0 0 1 0 0]
 [0 0 0 0 0 1 0 0 4 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 1 0 0 0 0]]
Agents: [{'position': (6, 4), 'role': 'leader'}, {'position': (4, 4), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
Episode 13: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 13: Average Reward: -0.01

Episode 13 finished with Cumulative Reward: 0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.4587114e-16, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.598503, shape=(), dtype=float32)

Episode 13 finished with Reward: -1
Leader Path: [(3, 3), (3, 3)]
Follower Path: [(5, 3), (4, 3)]


Episode 14/50
Starting reset process
Data: [[2 0 0 0 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 0 1]
 [0 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [4 0 1 0 0 0 0 1 2 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 2 3 0 3 0 0]
 [0 0 0 0 1 0 0 0 1 0]]
Agents: [{'position': (8, 7), 'role': 'leader'}, {'position': (8, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.8259815e-20, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.49786, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
Episode 14: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 14: Average Reward: -0.01

Episode 14 finished with Cumulative Reward: -11
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01026403  0.02150278  0.00352939  0.01948205 -0.01015675 -0.01625296
 -0.02962391 -0.01658937]
Reconstruction Loss tf.Tensor(1.1251712, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.816535e-20, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.522846, shape=(), dtype=float32)

Episode 14 finished with Reward: -1
Leader Path: [(8, 7), (7, 7)]
Follower Path: [(8, 5), (7, 5)]


Episode 15/50
Starting reset process
Data: [[0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 4 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [1 3 0 0 0 0 1 2 0 0]
 [0 1 0 0 0 0 0 2 0 0]
 [3 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 2 2]
 [0 0 0 0 0 0 0 0 1 0]
 [0 1 2 0 0 0 0 0 0 0]]
Agents: [{'position': (6, 0), 'role': 'leader'}, {'position': (4, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 15: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 15: Average Reward: 0.00

Episode 15 finished with Cumulative Reward: -1
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01369481  0.0226134   0.00579102  0.02079432 -0.01190857 -0.01366198
 -0.03071866 -0.01548924]
Reconstruction Loss tf.Tensor(1.376143, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.534654e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.098711, shape=(), dtype=float32)

Episode 15 finished with Reward: 0
Leader Path: [(6, 0)]
Follower Path: [(4, 1)]


Episode 16/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 1 0]
 [0 0 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 1 1]
 [0 0 0 2 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 1 0 0 0 0 0 1]
 [0 0 0 1 0 0 0 0 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 2 0 3 0 0 0 1]
 [0 0 0 0 3 0 0 0 0 1]]
Agents: [{'position': (8, 5), 'role': 'leader'}, {'position': (9, 4), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.1239708e-20, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.49819, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.825448e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.847126, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.8066235e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24607, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.3280316e-23, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.62001, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.3365726e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-98.04399, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 1 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 1 0 2]
 [0 0 0 0 1 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 2]
 [0 0 3 3 0 1 0 1 4 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 0 0 2 1 0 0 0 1]]
Agents: [{'position': (7, 3), 'role': 'leader'}, {'position': (7, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
Episode 16: Tether constraint violated (Distance: 4.12, Tether Limit: 2). Resetting...
Episode 16: Average Reward: -0.07

Episode 16 finished with Cumulative Reward: 0
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.0059464e-21, shape=(), dtype=float32)
Total Loss tf.Tensor(-98.06905, shape=(), dtype=float32)

Episode 16 finished with Reward: -15
Leader Path: [(8, 5), (7, 5), (6, 5), (5, 5), (4, 5), (3, 5)]
Follower Path: [(9, 4), (8, 4), (7, 4), (6, 4), (5, 4), (4, 4)]


Episode 17/50
Starting reset process
Data: [[0 2 0 1 0 1 0 1 0 0]
 [0 0 0 2 2 0 0 0 0 0]
 [0 0 0 0 0 1 0 1 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 3 0]
 [0 0 0 0 0 0 3 2 0 0]
 [0 0 0 0 1 4 1 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]]
Agents: [{'position': (5, 6), 'role': 'leader'}, {'position': (4, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.595644e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497597, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-13.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-212.8612, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.4867262e-23, shape=(), dtype=float32)
Total Loss tf.Tensor(-212.66158, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-14.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-229.23514, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.834728e-24, shape=(), dtype=float32)
Total Loss tf.Tensor(-229.0355, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-25.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-409.34848, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.662959e-23, shape=(), dtype=float32)
Total Loss tf.Tensor(-409.17392, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of Bounds
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-36.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-589.4618, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.0082631e-24, shape=(), dtype=float32)
Total Loss tf.Tensor(-589.26215, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of bounds
Starting reset process
Data: [[0 0 1 0 0 0 0 2 0 0]
 [1 0 0 0 0 0 1 0 0 4]
 [0 0 0 0 0 0 0 3 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 1 2 0 3 0 1 0 2]
 [0 0 0 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 1 0 0 2]
 [0 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (2, 7), 'role': 'leader'}, {'position': (4, 5), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Out of Bounds
follower
Episode 17: Hard obstacle constraint violated. Resetting...
Episode 17: Average Reward: -0.42

Episode 17 finished with Cumulative Reward: 0
loss tf.Tensor(-36.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-589.4618, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.8133816e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-589.31244, shape=(), dtype=float32)

Episode 17 finished with Reward: -85
Leader Path: [(5, 6), (4, 6), (3, 6), (2, 6), (1, 6), (0, 6)]
Follower Path: [(4, 8), (3, 7), (2, 7), (1, 7), (0, 7), (0, 7)]


Episode 18/50
Starting reset process
Data: [[0 0 3 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 3 0 0 1 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 2]
 [0 1 0 0 0 0 0 2 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 1 0 0 1 0]
 [1 0 0 0 0 2 0 0 1 0]
 [0 2 0 0 1 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 4 0]]
Agents: [{'position': (0, 2), 'role': 'leader'}, {'position': (2, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Out of bounds
Starting reset process
Data: [[0 1 0 1 0 2 0 1 0 0]
 [0 0 1 0 0 0 0 0 0 1]
 [0 0 0 0 3 1 0 2 0 1]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 3 0 0 4 0 0 0 0]
 [0 0 0 0 1 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 2 0 0 0 0]]
Agents: [{'position': (2, 4), 'role': 'leader'}, {'position': (4, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 18: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 18: Average Reward: 0.00

Episode 18 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.82842712  1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01935494  0.02164409  0.00847238  0.02165614 -0.01388615  0.00508908
 -0.03038892 -0.00802636]
Reconstruction Loss tf.Tensor(2.1243339, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.09283866e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-15.949073, shape=(), dtype=float32)

Episode 18 finished with Reward: 0
Leader Path: [(0, 2)]
Follower Path: [(2, 1)]


Episode 19/50
Starting reset process
Data: [[0 1 0 0 0 2 2 0 0 1]
 [2 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3]
 [0 0 1 1 0 0 0 0 0 0]
 [2 0 0 0 0 1 0 0 3 0]
 [2 0 0 0 0 4 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 1 0 0 0 0 0]]
Agents: [{'position': (4, 8), 'role': 'leader'}, {'position': (2, 9), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.         -1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.00546692  0.02066183  0.00281838  0.01479685 -0.01039413 -0.0455783
 -0.03101294 -0.02541097]
Reconstruction Loss tf.Tensor(1.2521544, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.326564e-22, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497448, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 19: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 19: Average Reward: -0.01

Episode 19 finished with Cumulative Reward: -2
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1. -1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.00348262  0.0194921   0.00132338  0.01382576 -0.00886737 -0.04266749
 -0.02978456 -0.02485357]
Reconstruction Loss tf.Tensor(1.1251042, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0018463e-21, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.522858, shape=(), dtype=float32)

Episode 19 finished with Reward: -1
Leader Path: [(4, 8), (3, 8)]
Follower Path: [(2, 9), (1, 8)]


Episode 20/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 2 0 4 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 3 1]
 [0 0 2 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 1 0 3]
 [2 0 0 1 0 0 0 0 1 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 2 0 1 0 0 0 0]]
Agents: [{'position': (3, 8), 'role': 'leader'}, {'position': (5, 9), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Out of tethered distance
Starting reset process
Data: [[0 0 0 0 3 0 0 0 0 0]
 [1 0 2 1 0 0 0 0 1 0]
 [0 1 3 0 0 0 0 0 0 0]
 [0 0 0 2 0 0 2 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 2 2 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 1 0 0 0 1 0 0]
 [0 0 1 0 0 1 4 0 0 0]]
Agents: [{'position': (0, 4), 'role': 'leader'}, {'position': (2, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
Episode 20: Tether constraint violated (Distance: 7.07, Tether Limit: 2). Resetting...
Episode 20: Average Reward: 0.00

Episode 20 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.5349827e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.12366, shape=(), dtype=float32)

Episode 20 finished with Reward: 0
Leader Path: [(3, 8)]
Follower Path: [(5, 9)]


Episode 21/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 1 2]
 [0 0 0 0 0 0 0 2 1 0]
 [0 0 0 0 1 0 3 0 0 1]
 [0 0 0 0 0 0 3 0 4 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 2]
 [1 0 0 0 2 0 0 0 0 0]
 [0 1 1 0 0 0 0 2 0 0]]
Agents: [{'position': (3, 6), 'role': 'leader'}, {'position': (2, 6), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Crash detected
Starting reset process
Data: [[2 0 0 0 0 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 1 0]
 [1 0 0 2 0 0 0 0 4 0]
 [0 0 1 0 0 0 0 0 0 0]
 [1 0 0 0 2 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 3 0 0 0 0 1]
 [0 1 1 0 3 0 0 1 0 0]]
Agents: [{'position': (9, 4), 'role': 'leader'}, {'position': (8, 4), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
Episode 21: Tether constraint violated (Distance: 7.28, Tether Limit: 2). Resetting...
Episode 21: Average Reward: 0.00

Episode 21 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.0080243   0.01689131 -0.00063591  0.01811047 -0.00769391  0.01200914
 -0.02407573 -0.00583469]
Reconstruction Loss tf.Tensor(1.2481482, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.23096e-26, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.12431, shape=(), dtype=float32)

Episode 21 finished with Reward: 0
Leader Path: [(3, 6)]
Follower Path: [(2, 6)]


Episode 22/50
Starting reset process
Data: [[0 0 0 0 0 0 2 0 1 0]
 [0 0 0 0 1 0 0 2 0 0]
 [0 0 0 1 0 0 0 1 0 0]
 [0 0 0 0 0 1 0 3 0 2]
 [1 0 0 0 0 0 3 0 0 0]
 [0 0 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 4]
 [2 0 2 0 0 1 0 0 0 0]]
Agents: [{'position': (4, 6), 'role': 'leader'}, {'position': (3, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.7705092e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.28763, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
Episode 22: Hard obstacle constraint violated. Resetting...
Episode 22: Average Reward: -0.06

Episode 22 finished with Cumulative Reward: -2
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.3099308e-24, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.31268, shape=(), dtype=float32)

Episode 22 finished with Reward: -11
Leader Path: [(4, 6), (3, 6)]
Follower Path: [(3, 7), (2, 7)]


Episode 23/50
Starting reset process
Data: [[0 0 0 0 0 0 0 1 0 0]
 [0 0 0 2 0 0 0 3 0 3]
 [0 1 0 1 0 0 0 0 0 0]
 [0 0 0 4 0 0 0 0 0 0]
 [0 0 0 2 1 0 0 0 0 0]
 [0 0 0 0 0 1 2 0 0 0]
 [0 0 0 1 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 1 0 0 0 0 0]
 [1 0 0 0 0 0 0 2 0 1]]
Agents: [{'position': (1, 9), 'role': 'leader'}, {'position': (1, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.308406e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.16234, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
Out of bounds
Starting reset process
Data: [[0 0 0 2 0 0 0 0 1 0]
 [0 1 0 0 0 2 0 0 0 1]
 [0 3 2 0 1 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 3 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 4 0 0 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 2 0 0 0 2 0]]
Agents: [{'position': (2, 1), 'role': 'leader'}, {'position': (4, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Out of Bounds
follower
Episode 23: Tether constraint violated (Distance: 6.32, Tether Limit: 2). Resetting...
Episode 23: Average Reward: -0.06

Episode 23 finished with Cumulative Reward: 0
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.777748e-30, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.1873, shape=(), dtype=float32)

Episode 23 finished with Reward: -11
Leader Path: [(1, 9), (0, 9)]
Follower Path: [(1, 7), (0, 7)]


Episode 24/50
Starting reset process
Data: [[0 0 0 0 0 0 1 2 0 0]
 [0 0 2 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 1]
 [0 1 0 2 0 0 0 2 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 3 0]
 [0 0 0 0 1 0 0 3 0 0]
 [0 0 4 0 0 0 0 1 0 1]
 [0 1 0 0 1 0 0 2 0 0]]
Agents: [{'position': (7, 7), 'role': 'leader'}, {'position': (6, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.694196e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.573303, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.916034e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.872128, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 53ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 53ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00689538  0.01692231 -0.00111776  0.01790977 -0.00714325  0.00902419
 -0.02400513 -0.0072077 ]
Reconstruction Loss tf.Tensor(1.1229975, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.6773395e-26, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.27116, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 1 0]
 [0 0 0 0 0 0 0 0 1 0]
 [2 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 2 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 1 2 2 1 0 0 0 0 0]
 [2 0 0 1 4 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 3 3]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (8, 9), 'role': 'leader'}, {'position': (8, 8), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
Episode 24: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 24: Average Reward: -0.03

Episode 24 finished with Cumulative Reward: 0
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.2268156e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.32118, shape=(), dtype=float32)

Episode 24 finished with Reward: -6
Leader Path: [(7, 7), (6, 7), (5, 7), (4, 7)]
Follower Path: [(6, 8), (5, 8), (5, 8), (5, 8)]


Episode 25/50
Starting reset process
Data: [[0 0 0 1 1 1 0 0 0 0]
 [0 1 0 0 2 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1]
 [0 0 0 0 0 4 0 0 2 0]
 [0 1 0 0 0 0 0 1 2 2]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 3 0 3 0 0 0 0]]
Agents: [{'position': (9, 3), 'role': 'leader'}, {'position': (9, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.2603837e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.44792, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.4345962e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.796898, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.82842712  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01772282  0.02072416  0.00692819  0.02106115 -0.01292651  0.00792667
 -0.02935293 -0.00695074]
Reconstruction Loss tf.Tensor(1.9988694, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.1741422e-34, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.095985, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.707613e-33, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.544785, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
mappo
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0012995e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-97.94367, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-7.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-114.61757, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.6932262e-33, shape=(), dtype=float32)
Total Loss tf.Tensor(-114.292656, shape=(), dtype=float32)
Update Policy
Step 7/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-8.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-130.99152, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.549351e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-130.7415, shape=(), dtype=float32)
Update Policy
Step 8/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 25: Hard obstacle constraint violated. Resetting...
Episode 25: Average Reward: -0.14

Episode 25 finished with Cumulative Reward: -8
loss tf.Tensor(-8.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-130.99152, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01026403  0.02150278  0.00352939  0.01948205 -0.01015675 -0.01625296
 -0.02962391 -0.01658937]
Reconstruction Loss tf.Tensor(1.1251712, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.4017714e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-130.76648, shape=(), dtype=float32)

Episode 25 finished with Reward: -28
Leader Path: [(9, 3), (8, 3), (7, 3), (6, 3), (5, 3), (4, 3), (3, 3), (2, 3)]
Follower Path: [(9, 5), (8, 5), (7, 5), (6, 5), (5, 5), (4, 5), (3, 5), (2, 5)]


Episode 26/50
Starting reset process
Data: [[0 0 0 0 1 1 0 0 1 1]
 [0 0 1 0 0 0 0 0 0 0]
 [1 0 0 1 2 2 0 0 0 0]
 [0 0 0 0 0 0 2 0 0 0]
 [0 0 0 0 4 3 0 1 0 0]
 [0 2 0 0 3 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (4, 5), 'role': 'leader'}, {'position': (5, 4), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.6867914e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.54824, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Hard obstacle encountered
Starting reset process
Data: [[3 0 0 1 1 0 0 0 2 0]
 [0 3 0 1 0 0 0 0 0 0]
 [2 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 4 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 1 1 0 2 0 0 0 0 0]
 [0 0 1 0 1 2 2 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (0, 0), 'role': 'leader'}, {'position': (1, 1), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 26: Tether constraint violated (Distance: 5.00, Tether Limit: 2). Resetting...
Episode 26: Average Reward: -0.01

Episode 26 finished with Cumulative Reward: 0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.344361e-27, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.573303, shape=(), dtype=float32)

Episode 26 finished with Reward: -1
Leader Path: [(4, 5), (3, 5)]
Follower Path: [(5, 4), (4, 4)]


Episode 27/50
Starting reset process
Data: [[0 0 1 1 1 0 0 0 0 0]
 [0 0 4 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 2 2]
 [0 1 1 0 0 0 0 0 1 0]
 [0 0 0 3 0 0 0 2 0 0]
 [0 0 0 0 3 0 0 0 0 1]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 2 2 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (5, 4), 'role': 'leader'}, {'position': (4, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.1325905e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.47319, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.0415365e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.847126, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00689538  0.01692231 -0.00111776  0.01790977 -0.00714325  0.00902419
 -0.02400513 -0.0072077 ]
Reconstruction Loss tf.Tensor(1.1229975, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.288743e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.27116, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
Episode 27: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 27: Average Reward: -0.03

Episode 27 finished with Cumulative Reward: -4
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.5368628e-30, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24607, shape=(), dtype=float32)

Episode 27 finished with Reward: -6
Leader Path: [(5, 4), (4, 4), (3, 4), (2, 4)]
Follower Path: [(4, 3), (3, 3), (3, 3), (3, 3)]


Episode 28/50
Starting reset process
Data: [[0 0 0 0 0 1 0 0 0 0]
 [0 0 0 2 1 0 0 0 0 0]
 [3 0 0 0 0 0 4 1 0 0]
 [3 1 2 0 0 0 1 0 0 0]
 [0 2 0 0 0 0 1 0 0 1]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 2 0 0 0 0]]
Agents: [{'position': (3, 0), 'role': 'leader'}, {'position': (2, 0), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Crash detected
Starting reset process
Data: [[0 0 0 0 0 0 0 0 2 0]
 [0 0 0 1 0 4 0 0 0 0]
 [0 0 3 0 0 2 0 0 0 0]
 [1 0 0 1 3 0 0 1 0 0]
 [0 1 0 2 1 0 1 0 0 0]
 [0 0 2 0 0 0 0 0 1 0]
 [0 0 0 1 0 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (3, 4), 'role': 'leader'}, {'position': (2, 2), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
Episode 28: Tether constraint violated (Distance: 4.47, Tether Limit: 2). Resetting...
Episode 28: Average Reward: 0.00

Episode 28 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.1940806e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.224564, shape=(), dtype=float32)

Episode 28 finished with Reward: 0
Leader Path: [(3, 0)]
Follower Path: [(2, 0)]


Episode 29/50
Starting reset process
Data: [[1 0 0 0 1 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 2 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 0]
 [1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 2 3 0 0 1 0]
 [0 0 0 3 0 0 0 0 4 0]
 [0 0 1 0 0 2 2 0 0 0]]
Agents: [{'position': (7, 5), 'role': 'leader'}, {'position': (8, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 29: Tether constraint violated (Distance: 3.16, Tether Limit: 2). Resetting...
Episode 29: Average Reward: 0.00

Episode 29 finished with Cumulative Reward: -1
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.7536375e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.12366, shape=(), dtype=float32)

Episode 29 finished with Reward: 0
Leader Path: [(7, 5)]
Follower Path: [(8, 3)]


Episode 30/50
Starting reset process
Data: [[1 0 0 0 1 1 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 0]
 [0 0 1 3 3 0 0 1 0 0]
 [0 1 0 0 0 0 0 0 0 2]
 [0 0 0 0 2 4 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 1 0 0 0 1 0]
 [0 0 2 0 0 0 0 0 2 0]]
Agents: [{'position': (4, 3), 'role': 'leader'}, {'position': (4, 4), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.1940806e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.598503, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.00420672  0.01683837 -0.00221348  0.01713603 -0.0058502   0.00147003
 -0.02385249 -0.01050412]
Reconstruction Loss tf.Tensor(0.87242365, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.6896271e-26, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.94733, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.0080243   0.01689131 -0.00063591  0.01811047 -0.00769391  0.01200914
 -0.02407573 -0.00583469]
Reconstruction Loss tf.Tensor(1.2481482, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.3530967e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24613, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.1458885e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.67006, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Out of bounds
Starting reset process
Data: [[0 0 2 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 0]
 [2 0 0 0 1 1 0 0 0 0]
 [0 0 0 1 0 0 0 0 2 1]
 [0 0 0 0 0 0 0 4 0 0]
 [0 0 2 0 0 0 0 0 0 0]
 [0 0 0 2 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [3 0 3 1 0 0 0 0 0 0]]
Agents: [{'position': (9, 2), 'role': 'leader'}, {'position': (9, 0), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
Episode 30: Tether constraint violated (Distance: 9.22, Tether Limit: 2). Resetting...
Episode 30: Average Reward: -0.05

Episode 30 finished with Cumulative Reward: 0
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.1592435e-27, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.69512, shape=(), dtype=float32)

Episode 30 finished with Reward: -10
Leader Path: [(4, 3), (3, 3), (2, 3), (1, 3), (0, 3)]
Follower Path: [(4, 4), (3, 4), (2, 4), (2, 4), (1, 4)]


Episode 31/50
Starting reset process
Data: [[0 0 0 2 0 0 0 0 0 0]
 [0 0 0 3 0 0 0 0 0 0]
 [0 0 0 3 0 1 0 0 1 1]
 [0 0 0 0 0 0 1 0 1 0]
 [0 1 0 0 0 0 4 0 2 0]
 [0 2 0 0 0 0 1 0 0 1]
 [0 0 0 0 0 1 0 0 0 1]
 [2 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0]]
Agents: [{'position': (1, 3), 'role': 'leader'}, {'position': (2, 3), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 1 0 0 4 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 1 0 0 0 1 0]
 [0 0 0 0 0 1 0 2 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [1 0 0 0 0 0 0 3 0 0]
 [1 0 1 0 0 0 0 0 0 2]
 [0 0 0 2 0 0 0 3 0 2]
 [0 0 0 1 0 0 0 0 1 0]]
Agents: [{'position': (8, 7), 'role': 'leader'}, {'position': (6, 7), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
Episode 31: Tether constraint violated (Distance: 8.06, Tether Limit: 2). Resetting...
Episode 31: Average Reward: 0.00

Episode 31 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.4016064e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.224564, shape=(), dtype=float32)

Episode 31 finished with Reward: 0
Leader Path: [(1, 3)]
Follower Path: [(2, 3)]


Episode 32/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 3 0 0 0 0 2 0 0 0]
 [0 0 0 0 0 0 0 2 2 0]
 [0 0 3 0 1 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0]
 [1 0 1 4 0 0 1 1 1 0]]
Agents: [{'position': (2, 1), 'role': 'leader'}, {'position': (4, 2), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Out of tethered distance
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 1 0 0 0 0 0 2 1 0]
 [0 3 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 1]
 [0 0 3 1 0 0 0 0 0 0]
 [0 0 0 0 2 1 4 2 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 1 0 0 0 0 2 0 0]
 [0 0 0 0 1 0 0 0 0 1]]
Agents: [{'position': (5, 2), 'role': 'leader'}, {'position': (3, 1), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.         -1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.00546692  0.02066183  0.00281838  0.01479685 -0.01039413 -0.0455783
 -0.03101294 -0.02541097]
Reconstruction Loss tf.Tensor(1.2521544, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.4350943e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497448, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Agent Collision
follower
Episode 32: Tether constraint violated (Distance: 0.00, Tether Limit: 2). Resetting...
Episode 32: Average Reward: -0.01

Episode 32 finished with Cumulative Reward: -1
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.6552617e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497597, shape=(), dtype=float32)

Episode 32 finished with Reward: -1
Leader Path: [(2, 1), (5, 2)]
Follower Path: [(4, 2), (4, 2)]


Episode 33/50
Starting reset process
Data: [[0 0 1 0 0 1 0 0 0 0]
 [0 1 0 2 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [2 0 1 1 3 0 0 0 0 0]
 [2 2 2 1 0 0 0 0 0 0]
 [0 0 0 1 0 0 3 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 4 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (4, 4), 'role': 'leader'}, {'position': (6, 6), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Out of tethered distance
Starting reset process
Data: [[2 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 1 1 0 0]
 [0 1 0 0 0 0 4 0 0 0]
 [2 0 0 0 0 0 0 0 0 3]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 1 3 0 0]
 [0 0 0 0 0 0 0 1 1 0]
 [2 0 0 0 1 0 0 0 0 0]
 [1 0 0 0 0 0 2 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]]
Agents: [{'position': (3, 9), 'role': 'leader'}, {'position': (5, 7), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
Episode 33: Tether constraint violated (Distance: 4.47, Tether Limit: 2). Resetting...
Episode 33: Average Reward: 0.00

Episode 33 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.82842712  0.
 -1.          0.        ]
decoded_message= [ 0.01686088  0.02525865  0.00916713  0.02193517 -0.01472717 -0.02680259
 -0.03339709 -0.01950858]
Reconstruction Loss tf.Tensor(1.6303084, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.953295e-34, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.047878, shape=(), dtype=float32)

Episode 33 finished with Reward: 0
Leader Path: [(4, 4)]
Follower Path: [(6, 6)]


Episode 34/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 1 0 0 0 0]
 [1 0 0 0 2 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 1 2]
 [0 0 0 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [3 0 0 0 0 0 0 0 0 0]
 [1 0 3 0 0 0 2 0 4 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 2]]
Agents: [{'position': (6, 0), 'role': 'leader'}, {'position': (7, 2), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.6552617e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497597, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.0116855e-30, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.872128, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.078122e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24607, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.4328094e-27, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.69512, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00241264  0.0166426  -0.00288888  0.01634422 -0.00501933 -0.00397102
 -0.02376446 -0.01273237]
Reconstruction Loss tf.Tensor(0.74687946, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.5228272e-25, shape=(), dtype=float32)
Total Loss tf.Tensor(-98.09425, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
Episode 34: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 34: Average Reward: -0.07

Episode 34 finished with Cumulative Reward: -33
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.2294637e-27, shape=(), dtype=float32)
Total Loss tf.Tensor(-98.06905, shape=(), dtype=float32)

Episode 34 finished with Reward: -15
Leader Path: [(6, 0), (5, 0), (4, 0), (3, 0), (3, 0), (3, 0)]
Follower Path: [(7, 2), (6, 1), (5, 1), (4, 1), (3, 1), (2, 1)]


Episode 35/50
Starting reset process
Data: [[0 0 0 0 0 0 2 0 1 0]
 [0 1 0 2 0 2 0 0 0 0]
 [0 0 1 2 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [3 4 1 0 0 0 1 0 0 0]
 [0 3 0 0 0 0 0 2 0 0]]
Agents: [{'position': (8, 0), 'role': 'leader'}, {'position': (9, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.9366937e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.49819, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 60ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 60ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.1003496e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.847126, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.026518e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24607, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.6545073e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.59501, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.3670843e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-97.99394, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
mappo
loss tf.Tensor(-7.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-114.61757, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.01096081  0.01873537  0.00193834  0.0192992  -0.00940315  0.00724877
 -0.02644272 -0.00769874]
Reconstruction Loss tf.Tensor(1.3734479, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.6686014e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-114.34288, shape=(), dtype=float32)
Update Policy
Step 7/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-8.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-130.99152, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.984943e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-130.79189, shape=(), dtype=float32)
Update Policy
Step 8/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-19.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-311.10486, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.640579e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-310.9303, shape=(), dtype=float32)
Update Policy
Step 9/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Out of bounds
Starting reset process
Data: [[0 2 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 1 0 0 3 0]
 [0 0 1 0 1 0 0 1 0 0]
 [0 0 0 0 0 0 3 0 0 0]
 [1 0 0 1 2 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [2 2 0 0 4 1 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (1, 8), 'role': 'leader'}, {'position': (3, 6), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
Episode 35: Tether constraint violated (Distance: 7.07, Tether Limit: 2). Resetting...
Episode 35: Average Reward: -0.23

Episode 35 finished with Cumulative Reward: 0
loss tf.Tensor(-19.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-311.10486, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.7014724e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-310.9052, shape=(), dtype=float32)

Episode 35 finished with Reward: -46
Leader Path: [(8, 0), (7, 0), (6, 0), (5, 0), (4, 0), (3, 0), (2, 0), (1, 0), (0, 0)]
Follower Path: [(9, 1), (8, 1), (7, 1), (6, 1), (5, 1), (4, 1), (3, 1), (2, 1), (1, 1)]


Episode 36/50
Starting reset process
Data: [[1 2 0 0 0 0 0 1 0 0]
 [0 2 0 1 0 0 0 0 0 0]
 [0 3 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 1 0 3 0 0 2 0 2 1]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [2 0 1 0 0 0 0 1 0 0]]
Agents: [{'position': (4, 3), 'role': 'leader'}, {'position': (2, 1), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 36: Tether constraint violated (Distance: 2.83, Tether Limit: 2). Resetting...
Episode 36: Average Reward: 0.00

Episode 36 finished with Cumulative Reward: -1
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.82842712  0.
 -1.          0.        ]
decoded_message= [ 0.01830248  0.02519376  0.00984737  0.02244014 -0.01503725 -0.02121839
 -0.03326441 -0.01769441]
Reconstruction Loss tf.Tensor(1.7545576, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0660984e-36, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.023027, shape=(), dtype=float32)

Episode 36 finished with Reward: 0
Leader Path: [(4, 3)]
Follower Path: [(2, 1)]


Episode 37/50
Starting reset process
Data: [[0 0 0 0 0 0 1 0 0 0]
 [1 0 0 1 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 4]
 [1 0 0 0 0 0 0 1 0 0]
 [1 0 0 0 0 0 2 3 0 3]
 [0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 2 1 0 0 0 0]
 [0 2 0 2 0 1 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (5, 9), 'role': 'leader'}, {'position': (5, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.9120035e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.16234, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
Valid Move
follower
Episode 37: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 37: Average Reward: -0.06

Episode 37 finished with Cumulative Reward: -1.0
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.0732054e-34, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.1873, shape=(), dtype=float32)

Episode 37 finished with Reward: -11
Leader Path: [(5, 9), (4, 9)]
Follower Path: [(5, 7), (4, 7)]


Episode 38/50
Starting reset process
Data: [[0 0 0 0 0 0 0 2 1 0]
 [0 2 0 0 1 0 1 0 0 1]
 [0 0 0 2 0 0 0 0 0 1]
 [3 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [3 0 0 0 0 0 0 0 0 0]
 [0 4 2 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 1 0 0 0 0 1 0 0]]
Agents: [{'position': (5, 0), 'role': 'leader'}, {'position': (3, 0), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.582052e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
Episode 38: Tether constraint violated (Distance: 3.00, Tether Limit: 2). Resetting...
Episode 38: Average Reward: -0.01

Episode 38 finished with Cumulative Reward: -1.0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.82842712  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01772282  0.02072416  0.00692819  0.02106115 -0.01292651  0.00792667
 -0.02935293 -0.00695074]
Reconstruction Loss tf.Tensor(1.9988694, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(9.501229e-37, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.348106, shape=(), dtype=float32)

Episode 38 finished with Reward: -1
Leader Path: [(5, 0), (4, 0)]
Follower Path: [(3, 0), (2, 0)]


Episode 39/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 1 0]
 [0 1 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [1 0 0 0 0 3 0 2 0 0]
 [0 2 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 3 0 4 0 0]
 [0 0 1 0 0 0 0 2 1 0]
 [1 0 0 0 0 0 0 2 0 1]
 [0 0 2 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (5, 5), 'role': 'leader'}, {'position': (3, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.2934295e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 39: Tether constraint violated (Distance: 3.00, Tether Limit: 2). Resetting...
Episode 39: Average Reward: -0.01

Episode 39 finished with Cumulative Reward: -1.0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.0400456e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)

Episode 39 finished with Reward: -1
Leader Path: [(5, 5), (4, 5)]
Follower Path: [(3, 5), (2, 5)]


Episode 40/50
Starting reset process
Data: [[0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 1 0 0 0 0 3 2 0 0]
 [1 0 1 0 0 0 0 0 0 1]
 [0 0 0 0 0 3 0 2 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 2 0 4 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 1]
 [0 0 1 0 0 0 0 0 2 0]]
Agents: [{'position': (4, 5), 'role': 'leader'}, {'position': (2, 6), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 40: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 40: Average Reward: 0.00

Episode 40 finished with Cumulative Reward: -1
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01597662  0.02239996  0.00684267  0.02136676 -0.01267599 -0.00581609
 -0.03058731 -0.01253686]
Reconstruction Loss tf.Tensor(1.6255889, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.1205395e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.048822, shape=(), dtype=float32)

Episode 40 finished with Reward: 0
Leader Path: [(4, 5)]
Follower Path: [(2, 6)]


Episode 41/50
Starting reset process
Data: [[0 0 0 4 0 2 0 0 0 2]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 1 0 0 0 0 2 0 0]
 [0 0 1 0 0 0 0 3 1 0]
 [0 0 1 0 0 0 0 0 3 0]
 [1 0 0 0 0 2 0 0 0 1]]
Agents: [{'position': (7, 7), 'role': 'leader'}, {'position': (8, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Hard obstacle encountered
Starting reset process
Data: [[1 0 0 0 0 0 1 0 0 3]
 [0 0 0 1 0 0 0 1 0 1]
 [0 0 0 0 0 0 0 0 3 0]
 [0 0 2 0 2 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 2 1 0 0 4 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 1]
 [0 2 2 0 0 0 0 0 0 0]]
Agents: [{'position': (0, 9), 'role': 'leader'}, {'position': (2, 8), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
Episode 41: Tether constraint violated (Distance: 7.07, Tether Limit: 2). Resetting...
Episode 41: Average Reward: 0.00

Episode 41 finished with Cumulative Reward: 0
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.6910395e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.199364, shape=(), dtype=float32)

Episode 41 finished with Reward: 0
Leader Path: [(7, 7)]
Follower Path: [(8, 8)]


Episode 42/50
Starting reset process
Data: [[0 0 0 1 0 0 0 0 1 0]
 [2 0 1 0 0 0 0 1 0 0]
 [0 0 0 0 1 0 1 0 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 3 0 0 0 0]
 [0 0 0 0 4 0 0 0 0 0]
 [0 0 0 0 0 3 0 0 0 0]
 [0 0 0 2 1 2 0 0 0 2]
 [0 0 0 0 0 1 0 0 1 1]
 [0 0 0 0 0 0 0 0 0 0]]
Agents: [{'position': (6, 5), 'role': 'leader'}, {'position': (4, 5), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 42: Tether constraint violated (Distance: 2.24, Tether Limit: 2). Resetting...
Episode 42: Average Reward: 0.00

Episode 42 finished with Cumulative Reward: -4
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  1.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01026403  0.02150278  0.00352939  0.01948205 -0.01015675 -0.01625296
 -0.02962391 -0.01658937]
Reconstruction Loss tf.Tensor(1.1251712, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(8.892066e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.148905, shape=(), dtype=float32)

Episode 42 finished with Reward: 0
Leader Path: [(6, 5)]
Follower Path: [(4, 5)]


Episode 43/50
Starting reset process
Data: [[0 0 0 0 1 1 0 1 0 0]
 [0 0 0 0 0 0 0 2 0 0]
 [0 2 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 4 0 1 0 0]
 [2 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 3 0 0]
 [0 0 0 0 1 2 0 3 0 0]
 [0 0 0 0 0 0 1 0 2 0]
 [0 0 0 0 1 1 0 0 0 0]]
Agents: [{'position': (6, 7), 'role': 'leader'}, {'position': (7, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  1.  0. -1.  0.]
decoded_message= [ 0.00689538  0.01692231 -0.00111776  0.01790977 -0.00714325  0.00902419
 -0.02400513 -0.0072077 ]
Reconstruction Loss tf.Tensor(1.1229975, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.9141863e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.52328, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Out of tethered distance
Starting reset process
Data: [[0 2 0 0 0 0 0 3 0 0]
 [0 0 0 0 0 0 0 2 0 0]
 [0 1 0 1 0 0 0 3 0 4]
 [0 0 0 2 1 0 2 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 1 0 0 1 0 0]
 [1 0 0 0 0 0 0 0 0 1]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]]
Agents: [{'position': (0, 7), 'role': 'leader'}, {'position': (2, 7), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 43: Tether constraint violated (Distance: 6.00, Tether Limit: 2). Resetting...
Episode 43: Average Reward: -0.01

Episode 43 finished with Cumulative Reward: 0
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(4.630426e-34, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.44792, shape=(), dtype=float32)

Episode 43 finished with Reward: -1
Leader Path: [(6, 7), (5, 7)]
Follower Path: [(7, 7), (7, 7)]


Episode 44/50
Starting reset process
Data: [[1 1 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 2 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 0 1]
 [0 1 0 0 0 0 1 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [4 2 3 2 0 0 2 0 0 0]
 [0 0 0 0 3 2 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (7, 2), 'role': 'leader'}, {'position': (8, 4), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Valid Move
follower
Episode 44: Hard obstacle constraint violated. Resetting...
Episode 44: Average Reward: 0.00

Episode 44 finished with Cumulative Reward: -1
loss tf.Tensor(-1.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-16.37394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.6832013e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-16.12366, shape=(), dtype=float32)

Episode 44 finished with Reward: 0
Leader Path: [(7, 2)]
Follower Path: [(8, 4)]


Episode 45/50
Starting reset process
Data: [[1 0 0 0 0 0 0 0 0 4]
 [0 0 0 0 0 0 0 2 0 2]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 3 0]
 [0 0 0 1 0 0 0 3 1 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 1 2 0 0 0 0 0 0 0]
 [1 1 0 0 2 0 0 0 0 0]
 [1 1 0 0 0 0 0 0 0 0]]
Agents: [{'position': (4, 7), 'role': 'leader'}, {'position': (3, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.5188555e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.573303, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.2435432e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.922176, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Hard obstacle encountered
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 1]
 [4 0 0 1 0 0 1 0 0 0]
 [0 1 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 3]
 [0 0 0 0 0 0 0 0 3 1]
 [2 0 0 0 2 0 1 1 0 0]
 [0 0 2 2 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]]
Agents: [{'position': (6, 8), 'role': 'leader'}, {'position': (5, 9), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 45: Tether constraint violated (Distance: 6.00, Tether Limit: 2). Resetting...
Episode 45: Average Reward: -0.01

Episode 45 finished with Cumulative Reward: 0
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(7.2302713e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.94724, shape=(), dtype=float32)

Episode 45 finished with Reward: -3
Leader Path: [(4, 7), (3, 7), (2, 7)]
Follower Path: [(3, 8), (2, 8), (1, 8)]


Episode 46/50
Starting reset process
Data: [[0 0 1 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [3 0 0 0 2 0 0 0 0 0]
 [1 0 3 0 0 0 0 1 0 0]
 [0 1 4 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 2 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 1 0 0 0]
 [0 0 0 0 0 0 2 1 0 0]]
Agents: [{'position': (2, 0), 'role': 'leader'}, {'position': (3, 2), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          2.23606798  0.
 -1.          0.        ]
decoded_message= [ 0.01214391  0.02259723  0.00511286  0.02019374 -0.01143016 -0.01923699
 -0.03080123 -0.0174484 ]
Reconstruction Loss tf.Tensor(1.2514038, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.43514e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.497597, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(47.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(769.57513, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.7670014e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(769.8248, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Out of bounds
Starting reset process
Data: [[0 0 0 0 0 1 0 0 0 0]
 [0 2 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 1 3 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 1 0 0 3 0]
 [2 1 2 0 1 0 0 0 0 0]
 [1 4 0 0 0 0 0 0 0 2]
 [0 0 0 0 0 1 0 0 0 0]]
Agents: [{'position': (6, 8), 'role': 'leader'}, {'position': (4, 8), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Valid Move
follower
Episode 46: Tether constraint violated (Distance: 9.22, Tether Limit: 2). Resetting...
Episode 46: Average Reward: 0.23

Episode 46 finished with Cumulative Reward: 0
loss tf.Tensor(47.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(769.57513, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00989406  0.01881257  0.00146597  0.01911461 -0.00892599  0.00420391
 -0.02642127 -0.00903415]
Reconstruction Loss tf.Tensor(1.2484275, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.7034225e-31, shape=(), dtype=float32)
Total Loss tf.Tensor(769.8248, shape=(), dtype=float32)

Episode 46 finished with Reward: 47
Leader Path: [(2, 0), (1, 0), (0, 0)]
Follower Path: [(3, 2), (2, 1), (1, 1)]


Episode 47/50
Starting reset process
Data: [[0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 1 0 0 0 0 0 2 2 0]
 [1 0 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 1 0 3 1]
 [0 0 0 0 2 0 2 0 0 0]
 [0 0 1 0 0 0 0 0 3 0]
 [0 0 0 0 4 0 0 1 0 0]]
Agents: [{'position': (8, 8), 'role': 'leader'}, {'position': (6, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.5245244e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.49786, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.4689696e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.871796, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.4193987e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.24574, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-5.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-81.8697, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.3751152e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-81.61968, shape=(), dtype=float32)
Update Policy
Step 5/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-6.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-98.24363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.3354476e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-97.993614, shape=(), dtype=float32)
Update Policy
Step 6/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-7.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-114.61757, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.299971e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-114.36755, shape=(), dtype=float32)
Update Policy
Step 7/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-8.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-130.99152, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.2681344e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-130.7415, shape=(), dtype=float32)
Update Policy
Step 8/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-9.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-147.36545, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.2395678e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-147.11543, shape=(), dtype=float32)
Update Policy
Step 9/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-10.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-163.7394, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.2139476e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-163.48938, shape=(), dtype=float32)
Update Policy
Step 10/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-11.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-180.11331, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.190865e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-179.8633, shape=(), dtype=float32)
Update Policy
Step 11/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-12.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-196.48726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.1701722e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-196.23724, shape=(), dtype=float32)
Update Policy
Step 12/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-13.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-212.8612, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.1515362e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-212.61119, shape=(), dtype=float32)
Update Policy
Step 13/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-14.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-229.23514, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.1347305e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-228.98512, shape=(), dtype=float32)
Update Policy
Step 14/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-15.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-245.60907, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.119632e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-245.35905, shape=(), dtype=float32)
Update Policy
Step 15/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-16.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-261.98303, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.106003e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-261.733, shape=(), dtype=float32)
Update Policy
Step 16/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-17.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-278.35696, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.093709e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-278.10693, shape=(), dtype=float32)
Update Policy
Step 17/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-18.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-294.7309, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.082638e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-294.48087, shape=(), dtype=float32)
Update Policy
Step 18/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-19.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-311.10486, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0726382e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-310.85483, shape=(), dtype=float32)
Update Policy
Step 19/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-20.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-327.4788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.063652e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-327.22876, shape=(), dtype=float32)
Update Policy
Step 20/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
mappo
loss tf.Tensor(-21.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-343.8527, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0555077e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-343.60266, shape=(), dtype=float32)
Update Policy
Step 21/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-22.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-360.22662, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0481756e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-359.9766, shape=(), dtype=float32)
Update Policy
Step 22/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-23.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-376.6006, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.041556e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-376.35056, shape=(), dtype=float32)
Update Policy
Step 23/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-24.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-392.97452, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0355537e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-392.7245, shape=(), dtype=float32)
Update Policy
Step 24/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-25.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-409.34848, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0301635e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-409.09845, shape=(), dtype=float32)
Update Policy
Step 25/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-26.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-425.7224, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.025291e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-425.47238, shape=(), dtype=float32)
Update Policy
Step 26/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-27.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-442.09634, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0208868e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-441.8463, shape=(), dtype=float32)
Update Policy
Step 27/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-28.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-458.47028, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.016903e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-458.22025, shape=(), dtype=float32)
Update Policy
Step 28/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-29.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-474.84424, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0133157e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-474.5942, shape=(), dtype=float32)
Update Policy
Step 29/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-30.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-491.21814, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0100766e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-490.9681, shape=(), dtype=float32)
Update Policy
Step 30/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-31.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-507.59213, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0071164e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-507.3421, shape=(), dtype=float32)
Update Policy
Step 31/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-32.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-523.96606, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.00448e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-523.71606, shape=(), dtype=float32)
Update Policy
Step 32/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-33.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-540.33997, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.0020745e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-540.08997, shape=(), dtype=float32)
Update Policy
Step 33/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-34.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-556.7139, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9998998e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-556.4639, shape=(), dtype=float32)
Update Policy
Step 34/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-35.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-573.08777, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.997933e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-572.83777, shape=(), dtype=float32)
Update Policy
Step 35/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-36.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-589.4618, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.996173e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-589.2118, shape=(), dtype=float32)
Update Policy
Step 36/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-37.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-605.8357, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9945736e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-605.5857, shape=(), dtype=float32)
Update Policy
Step 37/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-38.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-622.2097, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9931345e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-621.9597, shape=(), dtype=float32)
Update Policy
Step 38/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-39.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-638.5836, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.991834e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-638.3336, shape=(), dtype=float32)
Update Policy
Step 39/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
mappo
loss tf.Tensor(-40.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-654.9576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9906478e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-654.7076, shape=(), dtype=float32)
Update Policy
Step 40/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-41.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-671.3315, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9895983e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-671.0815, shape=(), dtype=float32)
Update Policy
Step 41/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
mappo
loss tf.Tensor(-42.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-687.7054, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.988618e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-687.4554, shape=(), dtype=float32)
Update Policy
Step 42/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-43.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-704.07935, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.987775e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-703.82935, shape=(), dtype=float32)
Update Policy
Step 43/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-44.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-720.45325, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9869544e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-720.20325, shape=(), dtype=float32)
Update Policy
Step 44/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-45.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-736.8273, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9862482e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-736.5773, shape=(), dtype=float32)
Update Policy
Step 45/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-46.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-753.2012, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9856105e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-752.9512, shape=(), dtype=float32)
Update Policy
Step 46/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-47.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-769.57513, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9849957e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-769.32513, shape=(), dtype=float32)
Update Policy
Step 47/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-48.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-785.94904, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9844946e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-785.69904, shape=(), dtype=float32)
Update Policy
Step 48/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-49.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-802.323, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.984017e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-802.073, shape=(), dtype=float32)
Update Policy
Step 49/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-50.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-818.69696, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9835845e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-818.44696, shape=(), dtype=float32)
Update Policy
Step 50/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-51.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-835.07086, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9831972e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-834.82086, shape=(), dtype=float32)
Update Policy
Step 51/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-52.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-851.4448, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9828563e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-851.1948, shape=(), dtype=float32)
Update Policy
Step 52/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-53.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-867.81866, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.982538e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-867.56866, shape=(), dtype=float32)
Update Policy
Step 53/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-54.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-884.1927, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9822424e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-883.9427, shape=(), dtype=float32)
Update Policy
Step 54/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
mappo
loss tf.Tensor(-55.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-900.56665, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.982015e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-900.31665, shape=(), dtype=float32)
Update Policy
Step 55/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-56.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-916.94055, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9817646e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-916.69055, shape=(), dtype=float32)
Update Policy
Step 56/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 50ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-57.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-933.31445, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.981537e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-933.06445, shape=(), dtype=float32)
Update Policy
Step 57/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-58.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-949.6885, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9813546e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-949.4385, shape=(), dtype=float32)
Update Policy
Step 58/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-59.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-966.0624, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9811733e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-965.8124, shape=(), dtype=float32)
Update Policy
Step 59/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-60.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-982.4363, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9810366e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-982.1863, shape=(), dtype=float32)
Update Policy
Step 60/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-61.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-998.81024, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980878e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-998.56024, shape=(), dtype=float32)
Update Policy
Step 61/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 29ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-62.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1015.18427, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980764e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1014.93427, shape=(), dtype=float32)
Update Policy
Step 62/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-63.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1031.5581, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980673e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1031.3081, shape=(), dtype=float32)
Update Policy
Step 63/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-64.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1047.9321, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9805597e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1047.6821, shape=(), dtype=float32)
Update Policy
Step 64/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-65.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1064.306, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9804456e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1064.056, shape=(), dtype=float32)
Update Policy
Step 65/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-66.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1080.6799, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980355e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1080.4299, shape=(), dtype=float32)
Update Policy
Step 66/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-67.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1097.054, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9802637e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1096.804, shape=(), dtype=float32)
Update Policy
Step 67/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-68.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1113.4279, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980173e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1113.1779, shape=(), dtype=float32)
Update Policy
Step 68/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-69.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1129.8019, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.980105e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1129.5519, shape=(), dtype=float32)
Update Policy
Step 69/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-70.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1146.1755, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9800595e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1145.9255, shape=(), dtype=float32)
Update Policy
Step 70/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-71.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1162.5497, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9800137e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1162.2997, shape=(), dtype=float32)
Update Policy
Step 71/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-72.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1178.9236, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9799913e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1178.6736, shape=(), dtype=float32)
Update Policy
Step 72/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-73.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1195.2975, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979946e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1195.0475, shape=(), dtype=float32)
Update Policy
Step 73/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-74.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1211.6714, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9799005e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1211.4214, shape=(), dtype=float32)
Update Policy
Step 74/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
mappo
loss tf.Tensor(-75.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1228.0454, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9799002e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1227.7954, shape=(), dtype=float32)
Update Policy
Step 75/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-76.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1244.4194, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979855e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1244.1694, shape=(), dtype=float32)
Update Policy
Step 76/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-77.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1260.7933, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979855e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1260.5433, shape=(), dtype=float32)
Update Policy
Step 77/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-78.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1277.1672, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979855e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1276.9172, shape=(), dtype=float32)
Update Policy
Step 78/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 50ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-79.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1293.5411, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979855e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1293.2911, shape=(), dtype=float32)
Update Policy
Step 79/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-80.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1309.9152, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798323e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1309.6652, shape=(), dtype=float32)
Update Policy
Step 80/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-81.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1326.2891, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798323e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1326.0391, shape=(), dtype=float32)
Update Policy
Step 81/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
mappo
loss tf.Tensor(-82.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1342.663, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798094e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1342.413, shape=(), dtype=float32)
Update Policy
Step 82/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-83.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1359.0369, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798094e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1358.7869, shape=(), dtype=float32)
Update Policy
Step 83/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-84.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1375.4108, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798094e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1375.1608, shape=(), dtype=float32)
Update Policy
Step 84/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-85.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1391.7848, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798094e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1391.5348, shape=(), dtype=float32)
Update Policy
Step 85/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
mappo
loss tf.Tensor(-86.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1408.1587, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9798094e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1407.9087, shape=(), dtype=float32)
Update Policy
Step 86/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-87.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1424.5327, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.979809e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1424.2827, shape=(), dtype=float32)
Update Policy
Step 87/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-88.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1440.9065, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797868e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1440.6565, shape=(), dtype=float32)
Update Policy
Step 88/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-89.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1457.2805, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1457.0305, shape=(), dtype=float32)
Update Policy
Step 89/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-90.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1473.6545, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1473.4045, shape=(), dtype=float32)
Update Policy
Step 90/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-91.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1490.0284, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1489.7784, shape=(), dtype=float32)
Update Policy
Step 91/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-92.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1506.4023, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1506.1523, shape=(), dtype=float32)
Update Policy
Step 92/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-93.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1522.7762, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1522.5262, shape=(), dtype=float32)
Update Policy
Step 93/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-94.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1539.1503, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1538.9003, shape=(), dtype=float32)
Update Policy
Step 94/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 29ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-95.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1555.5242, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1555.2742, shape=(), dtype=float32)
Update Policy
Step 95/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-96.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1571.8981, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1571.6481, shape=(), dtype=float32)
Update Policy
Step 96/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-97.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1588.272, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1588.022, shape=(), dtype=float32)
Update Policy
Step 97/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-98.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1604.646, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1604.396, shape=(), dtype=float32)
Update Policy
Step 98/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-99.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1621.02, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1620.77, shape=(), dtype=float32)
Update Policy
Step 99/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-100.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1637.3939, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1637.1439, shape=(), dtype=float32)
Update Policy
Step 100/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-101.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1653.7678, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1653.5178, shape=(), dtype=float32)
Update Policy
Step 101/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-102.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1670.1417, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1669.8917, shape=(), dtype=float32)
Update Policy
Step 102/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-103.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1686.5157, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1686.2657, shape=(), dtype=float32)
Update Policy
Step 103/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-104.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1702.8896, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1702.6396, shape=(), dtype=float32)
Update Policy
Step 104/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-105.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1719.2637, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1719.0137, shape=(), dtype=float32)
Update Policy
Step 105/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-106.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1735.6373, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1735.3873, shape=(), dtype=float32)
Update Policy
Step 106/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-107.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1752.0114, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1751.7614, shape=(), dtype=float32)
Update Policy
Step 107/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 52ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 52ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-108.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1768.3854, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1768.1354, shape=(), dtype=float32)
Update Policy
Step 108/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-109.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1784.7593, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1784.5093, shape=(), dtype=float32)
Update Policy
Step 109/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-110.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1801.1333, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1800.8833, shape=(), dtype=float32)
Update Policy
Step 110/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-111.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1817.5072, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1817.2572, shape=(), dtype=float32)
Update Policy
Step 111/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-112.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1833.8811, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1833.6311, shape=(), dtype=float32)
Update Policy
Step 112/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-113.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1850.255, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1850.005, shape=(), dtype=float32)
Update Policy
Step 113/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-114.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1866.6289, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1866.3789, shape=(), dtype=float32)
Update Policy
Step 114/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-115.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1883.003, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1882.753, shape=(), dtype=float32)
Update Policy
Step 115/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-116.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1899.377, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1899.127, shape=(), dtype=float32)
Update Policy
Step 116/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-117.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1915.7509, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1915.5009, shape=(), dtype=float32)
Update Policy
Step 117/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-118.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1932.1248, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1931.8748, shape=(), dtype=float32)
Update Policy
Step 118/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-119.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1948.4987, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1948.2487, shape=(), dtype=float32)
Update Policy
Step 119/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-120.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1964.8726, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1964.6226, shape=(), dtype=float32)
Update Policy
Step 120/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-121.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1981.2466, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1980.9966, shape=(), dtype=float32)
Update Policy
Step 121/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-122.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-1997.6205, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-1997.3705, shape=(), dtype=float32)
Update Policy
Step 122/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-123.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2013.9944, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2013.7444, shape=(), dtype=float32)
Update Policy
Step 123/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-124.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2030.3685, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2030.1185, shape=(), dtype=float32)
Update Policy
Step 124/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-125.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2046.7424, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2046.4924, shape=(), dtype=float32)
Update Policy
Step 125/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-126.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2063.1162, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2062.8662, shape=(), dtype=float32)
Update Policy
Step 126/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-127.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2079.4902, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2079.2402, shape=(), dtype=float32)
Update Policy
Step 127/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-128.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2095.8643, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2095.6143, shape=(), dtype=float32)
Update Policy
Step 128/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
mappo
loss tf.Tensor(-129.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2112.238, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2111.988, shape=(), dtype=float32)
Update Policy
Step 129/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-130.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2128.612, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2128.362, shape=(), dtype=float32)
Update Policy
Step 130/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-131.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2144.9858, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2144.7358, shape=(), dtype=float32)
Update Policy
Step 131/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-132.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2161.3599, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2161.1099, shape=(), dtype=float32)
Update Policy
Step 132/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-133.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2177.734, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2177.484, shape=(), dtype=float32)
Update Policy
Step 133/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-134.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2194.108, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2193.858, shape=(), dtype=float32)
Update Policy
Step 134/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
mappo
loss tf.Tensor(-135.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2210.4817, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2210.2317, shape=(), dtype=float32)
Update Policy
Step 135/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-136.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2226.8557, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2226.6057, shape=(), dtype=float32)
Update Policy
Step 136/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-137.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2243.2295, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2242.9795, shape=(), dtype=float32)
Update Policy
Step 137/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-138.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2259.6038, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2259.3538, shape=(), dtype=float32)
Update Policy
Step 138/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-139.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2275.9775, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2275.7275, shape=(), dtype=float32)
Update Policy
Step 139/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-140.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2292.351, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2292.101, shape=(), dtype=float32)
Update Policy
Step 140/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-141.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2308.7253, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2308.4753, shape=(), dtype=float32)
Update Policy
Step 141/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-142.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2325.0994, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2324.8494, shape=(), dtype=float32)
Update Policy
Step 142/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-143.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2341.4734, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2341.2234, shape=(), dtype=float32)
Update Policy
Step 143/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-144.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2357.8472, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2357.5972, shape=(), dtype=float32)
Update Policy
Step 144/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-145.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2374.2212, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2373.9712, shape=(), dtype=float32)
Update Policy
Step 145/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-146.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2390.595, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2390.345, shape=(), dtype=float32)
Update Policy
Step 146/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-147.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2406.969, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2406.719, shape=(), dtype=float32)
Update Policy
Step 147/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-148.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2423.3428, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2423.0928, shape=(), dtype=float32)
Update Policy
Step 148/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-149.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2439.7168, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2439.4668, shape=(), dtype=float32)
Update Policy
Step 149/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-150.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2456.0908, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2455.8408, shape=(), dtype=float32)
Update Policy
Step 150/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
mappo
loss tf.Tensor(-151.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2472.4648, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2472.2148, shape=(), dtype=float32)
Update Policy
Step 151/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step
mappo
loss tf.Tensor(-152.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2488.8389, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2488.5889, shape=(), dtype=float32)
Update Policy
Step 152/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-153.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2505.2126, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2504.9626, shape=(), dtype=float32)
Update Policy
Step 153/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-154.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2521.5867, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2521.3367, shape=(), dtype=float32)
Update Policy
Step 154/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-155.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2537.9604, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2537.7104, shape=(), dtype=float32)
Update Policy
Step 155/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-156.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2554.3345, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2554.0845, shape=(), dtype=float32)
Update Policy
Step 156/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-157.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2570.7083, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2570.4583, shape=(), dtype=float32)
Update Policy
Step 157/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-158.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2587.0823, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2586.8323, shape=(), dtype=float32)
Update Policy
Step 158/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-159.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2603.456, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2603.206, shape=(), dtype=float32)
Update Policy
Step 159/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-160.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2619.8303, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2619.5803, shape=(), dtype=float32)
Update Policy
Step 160/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-161.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2636.2043, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2635.9543, shape=(), dtype=float32)
Update Policy
Step 161/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-162.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2652.5781, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2652.3281, shape=(), dtype=float32)
Update Policy
Step 162/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-163.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2668.952, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2668.702, shape=(), dtype=float32)
Update Policy
Step 163/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-164.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2685.326, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2685.076, shape=(), dtype=float32)
Update Policy
Step 164/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-165.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2701.7, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2701.45, shape=(), dtype=float32)
Update Policy
Step 165/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-166.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2718.0737, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2717.8237, shape=(), dtype=float32)
Update Policy
Step 166/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-167.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2734.4478, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2734.1978, shape=(), dtype=float32)
Update Policy
Step 167/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
mappo
loss tf.Tensor(-168.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2750.8215, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2750.5715, shape=(), dtype=float32)
Update Policy
Step 168/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-169.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2767.1956, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2766.9456, shape=(), dtype=float32)
Update Policy
Step 169/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-170.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2783.5696, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2783.3196, shape=(), dtype=float32)
Update Policy
Step 170/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-171.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2799.9436, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2799.6936, shape=(), dtype=float32)
Update Policy
Step 171/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-172.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2816.3174, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2816.0674, shape=(), dtype=float32)
Update Policy
Step 172/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-173.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2832.6914, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2832.4414, shape=(), dtype=float32)
Update Policy
Step 173/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-174.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2849.0654, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2848.8154, shape=(), dtype=float32)
Update Policy
Step 174/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 30ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-175.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2865.4392, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2865.1892, shape=(), dtype=float32)
Update Policy
Step 175/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-176.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2881.813, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2881.563, shape=(), dtype=float32)
Update Policy
Step 176/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
mappo
loss tf.Tensor(-177.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2898.187, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2897.937, shape=(), dtype=float32)
Update Policy
Step 177/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-178.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2914.561, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2914.311, shape=(), dtype=float32)
Update Policy
Step 178/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-179.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2930.935, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2930.685, shape=(), dtype=float32)
Update Policy
Step 179/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
mappo
loss tf.Tensor(-180.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2947.309, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2947.059, shape=(), dtype=float32)
Update Policy
Step 180/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-181.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2963.6829, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2963.4329, shape=(), dtype=float32)
Update Policy
Step 181/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-182.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2980.057, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2979.807, shape=(), dtype=float32)
Update Policy
Step 182/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-183.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-2996.4307, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-2996.1807, shape=(), dtype=float32)
Update Policy
Step 183/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
mappo
loss tf.Tensor(-184.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3012.8047, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3012.5547, shape=(), dtype=float32)
Update Policy
Step 184/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-185.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3029.1787, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3028.9287, shape=(), dtype=float32)
Update Policy
Step 185/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-186.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3045.5525, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3045.3025, shape=(), dtype=float32)
Update Policy
Step 186/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-187.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3061.9268, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3061.6768, shape=(), dtype=float32)
Update Policy
Step 187/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 48ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
mappo
loss tf.Tensor(-188.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3078.3005, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3078.0505, shape=(), dtype=float32)
Update Policy
Step 188/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-189.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3094.6746, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3094.4246, shape=(), dtype=float32)
Update Policy
Step 189/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-190.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3111.0483, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3110.7983, shape=(), dtype=float32)
Update Policy
Step 190/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 43ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step
mappo
loss tf.Tensor(-191.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3127.4224, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3127.1724, shape=(), dtype=float32)
Update Policy
Step 191/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-192.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3143.7961, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3143.5461, shape=(), dtype=float32)
Update Policy
Step 192/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-193.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3160.1702, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3159.9202, shape=(), dtype=float32)
Update Policy
Step 193/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-194.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3176.544, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3176.294, shape=(), dtype=float32)
Update Policy
Step 194/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
mappo
loss tf.Tensor(-195.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3192.918, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3192.668, shape=(), dtype=float32)
Update Policy
Step 195/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
mappo
loss tf.Tensor(-196.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3209.292, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3209.042, shape=(), dtype=float32)
Update Policy
Step 196/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-197.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3225.6663, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3225.4163, shape=(), dtype=float32)
Update Policy
Step 197/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
mappo
loss tf.Tensor(-198.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3242.04, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3241.79, shape=(), dtype=float32)
Update Policy
Step 198/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-199.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3258.4136, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3258.1636, shape=(), dtype=float32)
Update Policy
Step 199/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-200.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3274.7878, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3274.5378, shape=(), dtype=float32)
Update Policy
Step 200/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step
Obstacle Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-201.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3291.1616, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3290.9116, shape=(), dtype=float32)
Update Policy
Episode 47: Average Reward: -100.50

Episode 47 finished with Cumulative Reward: -1.0
loss tf.Tensor(-201.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-3291.1616, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01186006  0.02155166  0.00420724  0.02012094 -0.01070243 -0.01069168
 -0.0295669  -0.01457453]
Reconstruction Loss tf.Tensor(1.2500875, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.9797645e-32, shape=(), dtype=float32)
Total Loss tf.Tensor(-3290.9116, shape=(), dtype=float32)

Episode 47 finished with Reward: -20100
Leader Path: [(8, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8), (7, 8)]
Follower Path: [(6, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8), (5, 8)]


Episode 48/50
Starting reset process
Data: [[0 0 0 0 0 0 0 0 0 0]
 [1 1 0 0 0 0 0 0 0 0]
 [2 0 0 0 0 0 0 0 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 3 0 3 0]
 [1 0 0 0 1 0 0 0 0 1]
 [0 0 0 0 2 0 2 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 2 1 0]
 [0 0 0 0 1 0 0 0 0 4]]
Agents: [{'position': (4, 6), 'role': 'leader'}, {'position': (4, 8), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 50ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1.  2.  1.  2.  0. -1.  0.]
decoded_message= [ 0.01421751  0.02139213  0.0052683   0.02073586 -0.01156979 -0.00288459
 -0.02947879 -0.01154965]
Reconstruction Loss tf.Tensor(1.4997821, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.5549787e-34, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.44792, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
Valid Move
follower
Episode 48: Hard obstacle constraint violated. Resetting...
Episode 48: Average Reward: -0.01

Episode 48 finished with Cumulative Reward: -2
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          2.          0.
 -1.          0.        ]
decoded_message= [ 0.01519774  0.02125328  0.00572489  0.02089177 -0.01194369  0.00022924
 -0.02944228 -0.01027547]
Reconstruction Loss tf.Tensor(1.6245841, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(5.966272e-35, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.422962, shape=(), dtype=float32)

Episode 48 finished with Reward: -1
Leader Path: [(4, 6), (3, 6)]
Follower Path: [(4, 8), (3, 8)]


Episode 49/50
Starting reset process
Data: [[0 0 0 0 4 0 0 0 0 0]
 [0 0 2 0 1 1 1 0 2 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 2 0 0]
 [0 1 0 0 0 0 1 0 0 0]
 [2 0 0 0 1 0 0 0 3 0]
 [0 0 1 0 0 0 0 3 1 0]]
Agents: [{'position': (8, 8), 'role': 'leader'}, {'position': (9, 7), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.0487277e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.573303, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 46ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[79.99999]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048651e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00734406  0.01883303  0.00038289  0.01840146 -0.00780967 -0.0034717
 -0.0263796  -0.01222606]
Reconstruction Loss tf.Tensor(0.99818295, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(2.638635e-29, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.922176, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
Episode 49: Hard obstacle constraint violated. Resetting...
Episode 49: Average Reward: -0.01

Episode 49 finished with Cumulative Reward: -3
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [ 0.00563307  0.01870025 -0.00029416  0.01767023 -0.00709692 -0.00897441
 -0.02635892 -0.01437111]
Reconstruction Loss tf.Tensor(0.8728745, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.0487277e-28, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.94724, shape=(), dtype=float32)

Episode 49 finished with Reward: -3
Leader Path: [(8, 8), (7, 8), (6, 8)]
Follower Path: [(9, 7), (8, 7), (7, 7)]


Episode 50/50
Starting reset process
Data: [[0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 2 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 3 3 0 0 0 1 0 0 1]
 [2 0 2 0 0 0 0 4 0 0]
 [0 2 1 0 0 0 0 1 0 0]
 [1 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [0 1 0 0 2 0 0 0 0 0]]
Agents: [{'position': (3, 1), 'role': 'leader'}, {'position': (3, 2), 'role': 'follower'}]
Step 1/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 31ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-2.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-32.74788, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          1.41421356  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.00420672  0.01683837 -0.00221348  0.01713603 -0.0058502   0.00147003
 -0.02385249 -0.01050412]
Reconstruction Loss tf.Tensor(0.87242365, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(3.2097853e-27, shape=(), dtype=float32)
Total Loss tf.Tensor(-32.573395, shape=(), dtype=float32)
Update Policy
Step 2/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Agent Collision
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
mappo
loss tf.Tensor(-3.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-49.121815, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.          2.23606798  1.          1.          0.
 -1.          0.        ]
decoded_message= [ 0.0080243   0.01689131 -0.00063591  0.01811047 -0.00769391  0.01200914
 -0.02407573 -0.00583469]
Reconstruction Loss tf.Tensor(1.2481482, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.416315e-30, shape=(), dtype=float32)
Total Loss tf.Tensor(-48.872185, shape=(), dtype=float32)
Update Policy
Step 3/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
Valid Move
follower
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step
mappo
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1.         -1.         -1.          1.          1.41421356  0.
 -1.          0.        ]
decoded_message= [-0.00141511  0.0165085  -0.00228688  0.01135618 -0.00518181 -0.03547552
 -0.02634881 -0.02336169]
Reconstruction Loss tf.Tensor(0.8709742, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(1.19979e-21, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.32156, shape=(), dtype=float32)
Update Policy
Step 4/200
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step
Out of bounds
Starting reset process
Data: [[0 0 0 0 1 0 0 1 0 1]
 [0 0 0 0 0 2 0 0 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 1 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 1 0 0 1 0]
 [0 0 0 0 0 0 3 0 0 0]
 [0 0 0 0 4 0 0 2 3 2]
 [0 0 0 1 0 0 2 0 0 0]
 [0 0 0 0 0 0 0 0 2 0]]
Agents: [{'position': (7, 8), 'role': 'leader'}, {'position': (6, 6), 'role': 'follower'}]
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 32ms/step
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2025-03-30 17:02:12.067959: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:68] Profiler session collecting data.
2025-03-30 17:02:12.097613: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1036] CUPTI activity buffer flushed
2025-03-30 17:02:14.183279: I external/local_xla/xla/backends/profiler/gpu/cupti_collector.cc:534]  GpuTracer has collected 114452 callback api events and 109740 activity events. 
2025-03-30 17:02:17.038457: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.
2025-03-30 17:02:17.045886: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:147] Collecting XSpace to repository: logs/plugins/profile/2025_03_30_17_02_17/scs-gpu-comp5801G-03.xplane.pb
Valid Move
follower
Episode 50: Tether constraint violated (Distance: 9.90, Tether Limit: 2). Resetting...
Episode 50: Average Reward: -0.03

Episode 50 finished with Cumulative Reward: 0
loss tf.Tensor(-4.0, shape=(), dtype=float32)
Policy Gradient Loss tf.Tensor(-65.49576, shape=(), dtype=float32)
labels tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
sim_matrix tf.Tensor([[80.]], shape=(1, 1), dtype=float32)
Contrastive Loss tf.Tensor(1.8048515e-35, shape=(), dtype=float32)
leader_message=[-1. -1. -1.  1.  0.  0. -1.  0.]
decoded_message= [-0.01280497  0.00872386 -0.01019021  0.00513443  0.00311728 -0.01871903
 -0.01588659 -0.0190313 ]
Reconstruction Loss tf.Tensor(0.61634505, shape=(), dtype=float32)
Entropy Bonus tf.Tensor(6.9763124e-15, shape=(), dtype=float32)
Total Loss tf.Tensor(-65.37249, shape=(), dtype=float32)

Episode 50 finished with Reward: -6
Leader Path: [(3, 1), (2, 1), (1, 1), (0, 1)]
Follower Path: [(3, 2), (2, 2), (2, 2), (1, 1)]

Training logs exported to 'evaluation_metrics.csv'
Success Rate: 0
Best models are saved.
Time taken for this episode: 197.06048488616943 seconds
Time taken for hyperparameter tuning and training with the best ones: 197.0605127811432 seconds
Time taken for the entire pipeline process is: 197.12865710258484 seconds
Best Hyperparameters: {'lr': 0.006435, 'episodes': 50}
Best Success Rate: 0
âœ… Completed the training pipeline successfully.
