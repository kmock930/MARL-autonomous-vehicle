# -*- coding: utf-8 -*-
"""MARL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSK3DT299xLss5u2fcOZsYO7y2oa0Pgr
"""

# # Define the Actor (Policy Network)
# class Actor(tf.keras.Model):
#     def __init__(self, num_actions):
#         super(Actor, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(num_actions, activation="softmax")

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

# # Define the Critic (Value Network)
# class Critic(tf.keras.Model):
#     def __init__(self):
#         super(Critic, self).__init__()
#         self.dense1 = tf.keras.layers.Dense(64, activation="relu")
#         self.dense2 = tf.keras.layers.Dense(64, activation="relu")
#         self.output_layer = tf.keras.layers.Dense(1)

#     def call(self, inputs):
#         x = self.dense1(inputs)
#         x = self.dense2(x)
#         return self.output_layer(x)

import tensorflow as tf
import numpy as np
import networkx as nx
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from enum import Enum
from collections import deque
import random

FREE: int = 0
OBSTACLE_SOFT: int = 1
OBSTACLE_HARD: int = 2
LEADER: int = 3
FOLLOWER: int = 4
TARGET: int = 5

class ACTION_SPACE(Enum):
    UP = (0, -1)
    DOWN = (0, 1)
    LEFT = (-1, 0)
    RIGHT = (1, 0)
    STAY = (0, 0)
    # UP_LEFT = (-1, -1)
    # UP_RIGHT = (1, -1)
    # DOWN_LEFT = (-1, 1)
    # DOWN_RIGHT = (1, 1)

def move_agent(agent_position, action):
  x,y = agent_position[0]
  dx,dy = action.value

  new_pos = (x + dx, y+ dy)

  if env[new_pos] == LEADER or env[new_pos] == FOLLOWER or env[new_pos] ==OBSTACLE_SOFT or env[new_pos] ==OBSTACLE_HARD or (0 <= new_pos[0] < 10 and 0 <= new_pos[1] < 10):
    return agent_position

  elif env[new_pos] == 0 or env[new_pos] == 4:
    env[new_pos] == AGENT
    env[agent_position] = FREE

  return new_pos

# def calculate_reward(env, leader_pos, follower_pos, target):
#   reward = 0
#   if env[leader_pos] == target or env[follower_pos] == target:
#     reward = 50
#   elif env[leader_pos] == OBSTACLE_SOFT or env[follower_pos] == OBSTACLE_SOFT:
#     reward = -0
#   return reward

#window: 3X3
def get_nearest_obstacle_distance(pos):
  x,y = pos[0]
  distances = []

  for dx in range(-1, 2):
      for dy in range(-1, 2):
            nx, ny = x + dx, y + dy

            if 0 <= nx < 10 and 0 <= ny < 10 and env[nx, ny] in [OBSTACLE_SOFT, OBSTACLE_HARD]:
                distances.append((dx, dy))
  return min(distances, key=lambda d: abs(d[0]) + abs(d[1])) if distances else (0, 0)

def generate_leader_message(leader_pos):
   obs_x,obs_y =get_nearest_obstacle_distance(leader_pos)
   leader_action = random.choice(list(ACTION_SPACE))

   print()
   return np.array([obs_x, obs_y, leader_pos[0][0], leader_pos[0][1], leader_action.value[0], leader_action.value[1]])

# LSTM
from tensorflow.keras.layers import Reshape

def build_encoder_decoder():
    input_layer = Input(shape=(6,))
    reshaped = Reshape((1, 6))(input_layer)
    x = LSTM(64, return_sequences=True)(reshaped)
    x = LSTM(32)(x)
    output_layer = Dense(6, activation="linear")(x)
    return Model(input_layer, output_layer)

encoder_decoder = build_encoder_decoder()

# MLP MAPPO
def build_policy_network():
    input_layer = Input(shape=(6,))
    x = Dense(64, activation="relu")(input_layer)
    x = Dense(64, activation="relu")(x)
    output_layer = Dense(len(ACTION_SPACE), activation="softmax")(x)
    return Model(input_layer, output_layer)

leader_policy = build_policy_network()
follower_policy = build_policy_network()

class MAPPO:
  def __init__ (self, leader_model, follower_model, encoded_model, lr=0.001):
    self.leader_model = leader_model
    self.follower_model = follower_model
    self.encoded_model = encoded_model

    self.optimizers = Adam(learning_rate=lr)

  def compute_loss(self, state_leader, decoded_msg, action_leader, action_follower, reward):
      action_prob_leader = self.leader_model(state_leader.reshape(1, -1))
      action_prob_follower = self.follower_model(decoded_msg.reshape(1, -1))
      loss = -tf.reduce_mean(reward * tf.math.log(action_prob_leader)) - tf.reduce_mean(reward * tf.math.log(action_prob_follower))
      return loss

  def apply_gradients(self, state_leader, decoded_msg, action_leader, action_follower, reward):
        with tf.GradientTape() as tape:
            loss = self.compute_loss(state_leader, decoded_msg, action_leader, action_follower, reward)

        grads = tape.gradient(loss, self.leader_model.trainable_variables + self.follower_model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.leader_model.trainable_variables + self.follower_model.trainable_variables))

def train_MAPPO(episodes, leader_model, follower_model, encoded_model, leader_pos, target_pos,follower_pos):

  optimizer = Adam(learning_rate=lr)

  for episode in range(episodes):
    # reset the environment
    env[:] = FREE
    env[leader_pos] = LEADER
    env[follower_pos] = FOLLOWER
    env[target_pos] = TARGET

    total_reward = 0
    leader_path = [leader_pos]
    follower_path = [follower_pos]
    episode_reset = False

    for step in range(2):

        # Leader moves
        l_msg = generate_leader_message(leader_pos)
        leader_action_probs = leader_model.predict(l_msg.reshape(1,-1))
        leader_action = list(ACTION_SPACE)[np.argmax(leader_action_probs)]
        new_leader_pos = move_agent(leader_pos, leader_action)

        # encoded decoded
        encoded_msg = encoded_model.predict(l_msg.reshape(1,-1))
        decoded_msg = encoded_msg.reshape(-1)

        # Follower moves
        follower_action_probs = follower_model.predict(decoded_msg.reshape(1,-1))
        follower_action = list(ACTION_SPACE)[np.argmax(follower_action_probs)]
        new_follower_pos = move_agent(follower_pos, follower_action)

        # compute distance
        distance = np.sqrt((new_leader_pos[0][0] - new_follower_pos[0][0])**2 + (new_leader_pos[0][1] - new_follower_pos[0][1])**2)

        x_l , y_l = new_leader_pos[0]
        x_f , y_f = new_follower_pos[0]

        if distance > 2 or distance < 1:
          print(f"Episode {episode+1}: Distance constraint violated (Distance: {distance:.2f}). Resetting...")
          break


        elif env[x_l, y_l] == OBSTACLE_HARD or env[x_f , y_f] == OBSTACLE_HARD:
          print(f"Episode {episode+1}: Hard obstacle constraint violated. Resetting...")
          break

        # update the path and position
        follower_pos = new_follower_pos
        follower_path.append(follower_pos)

        leader_pos = new_leader_pos
        leader_path.append(leader_pos)


        # compute reward
        reward = 0
        if env[x_l, y_l] == TARGET or env[x_f , y_f] == TARGET:
          reward += 10
        elif env[x_l, y_l] == OBSTACLE_SOFT or env[x_f , y_f] == OBSTACLE_SOFT:
          reward -= 2

        total_reward += reward

        with tf.GradientTape() as tape:
            action_prob_leader = leader_model(l_msg.reshape(1, -1))
            action_prob_follower = follower_model(decoded_msg.reshape(1, -1))
            loss = -tf.reduce_mean(reward * tf.math.log(action_prob_leader)) - tf.reduce_mean(reward * tf.math.log(action_prob_follower))

        grads = tape.gradient(loss, leader_model.trainable_variables + follower_model.trainable_variables)
        optimizer.apply_gradients(zip(grads, leader_model.trainable_variables + follower_model.trainable_variables))

  if not episode_reset:
            print(f"\nEpisode {episode+1} finished with Reward: {total_reward}")
            print(f"Leader Path: {leader_path}")
            print(f"Follower Path: {follower_path}\n")

env = np.array([
    [0, 0, 5, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 3, 4, 0, 0, 0, 0, 1, 1],
    [0, 0, 0, 0, 0, 0, 2, 0, 0, 1],
    [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 2, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 2, 0],
    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 2, 0, 0, 0, 0, 0, 0, 1],
    [0, 1, 0, 0, 1, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 2, 0, 0]
  ])

leader_pos= np.argwhere(env == LEADER)
follower_pos= np.argwhere(env == FOLLOWER)
target_pos = np.argwhere(env == TARGET)

lr = 0.001

print(leader_pos)
print(follower_pos)
print(target_pos)

train_MAPPO(2, leader_policy, follower_policy, encoder_decoder,leader_pos, target_pos, follower_pos,)

x,y = leader_pos[0]
env[x,y]

