{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # Define the Actor (Policy Network)\n",
        "# class Actor(tf.keras.Model):\n",
        "#     def __init__(self, num_actions):\n",
        "#         super(Actor, self).__init__()\n",
        "#         self.dense1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "#         self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "#         self.output_layer = tf.keras.layers.Dense(num_actions, activation=\"softmax\")\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         x = self.dense1(inputs)\n",
        "#         x = self.dense2(x)\n",
        "#         return self.output_layer(x)"
      ],
      "metadata": {
        "id": "VGuM_yoM6dIQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the Critic (Value Network)\n",
        "# class Critic(tf.keras.Model):\n",
        "#     def __init__(self):\n",
        "#         super(Critic, self).__init__()\n",
        "#         self.dense1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "#         self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "#         self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         x = self.dense1(inputs)\n",
        "#         x = self.dense2(x)\n",
        "#         return self.output_layer(x)"
      ],
      "metadata": {
        "id": "R8TElU5sDj0L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DTIMEhlCZqQk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from enum import Enum\n",
        "from collections import deque\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FREE: int = 0\n",
        "OBSTACLE_SOFT: int = 1\n",
        "OBSTACLE_HARD: int = 2\n",
        "# AGENT: int = 3\n",
        "LEADER: int = 3\n",
        "FOLLOWER: int = 5\n",
        "TARGET: int = 4\n"
      ],
      "metadata": {
        "id": "5WHuroehGUbx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agents: np.ndarray | list[dict]\n",
        "agents = [\n",
        "    {\"id\": 0, \"role\": \"leader\"},\n",
        "    {\"id\": 1, \"role\": \"follower\"}\n",
        "]"
      ],
      "metadata": {
        "id": "7jfD_rwlevP6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ACTION_SPACE(Enum):\n",
        "    UP = (0, -1)\n",
        "    DOWN = (0, 1)\n",
        "    LEFT = (-1, 0)\n",
        "    RIGHT = (1, 0)\n",
        "    STAY = (0, 0)\n",
        "    UP_LEFT = (-1, -1)\n",
        "    UP_RIGHT = (1, -1)\n",
        "    DOWN_LEFT = (-1, 1)\n",
        "    DOWN_RIGHT = (1, 1)\n"
      ],
      "metadata": {
        "id": "jAGaz6dJq8JP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnol\n",
        "def new_pos(agent_position, action):\n",
        "  x,y = agent_position[0]\n",
        "  dx,dy = action.value\n",
        "\n",
        "  new_pos = (x + dx, y+ dy)\n",
        "\n",
        "  if not (0 <= new_pos[0] < 10 and 0 <= new_pos[1] < 10):\n",
        "        return agent_position\n",
        "\n",
        "  if env[new_pos] in [LEADER, FOLLOWER, OBSTACLE_SOFT, OBSTACLE_HARD]:\n",
        "    return agent_position\n",
        "\n",
        "  # elif env[new_pos] == 0 or env[new_pos] == 4:\n",
        "  #   env[new_pos] == AGENT\n",
        "  #   env[agent_position] = FREE\n",
        "\n",
        "  return new_pos"
      ],
      "metadata": {
        "id": "pCfpI9_dq-N-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_reward(env, leader_pos, follower_pos, target):\n",
        "#   reward = 0\n",
        "#   if env[leader_pos] == target or env[follower_pos] == target:\n",
        "#     reward = 50\n",
        "#   elif env[leader_pos] == OBSTACLE_SOFT or env[follower_pos] == OBSTACLE_SOFT:\n",
        "#     reward = -0\n",
        "#   return reward\n"
      ],
      "metadata": {
        "id": "8fWKwojhrawW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpcAjK3z-gSs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message structure :\n",
        "\n",
        "# - Distance to the nearest obstacle (obs_dist): int or float\n",
        "# - Relative position of the goal (xg): int, -1 if goal is not in partial observability.\n",
        "# - Relative position of the goal (yg): int, -1 if goal is not in partial observability.\n",
        "# - Whether the path is clear or blocked(path_blocked): bool or 0/1 int\n",
        "# - Leader's action (action.name): str\n",
        "# - Leader can observe the follower or not (follower_visibility): bool or 0/1 int\n",
        "# - Leaders distance to follower (follower_dist): int or float\n"
      ],
      "metadata": {
        "id": "Fs88i7malco2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Suggested movement: e.g., (0,0) if leader suggests follower to stay or (0,-1) if suggested going left. Not the coordinates here!\n",
        "# - Urgency level: int ranging between 1-5, the lower the more urgent."
      ],
      "metadata": {
        "id": "OEACjIF2layQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import IsNot\n",
        "#window: 3X3\n",
        "def get_leader_message(pos):\n",
        "  # message = []\n",
        "  follower_visibility = 0 #leader cannot observe it\n",
        "  follower_dist, obs_dist, counter = -1,-1, 0\n",
        "  obstacles_pos, distances = [],[]\n",
        "  path_blocked = 0 #not blocked\n",
        "  x,y = pos[0]\n",
        "  xg,yg = (-1,-1)\n",
        "\n",
        "\n",
        "  for dx in range(-2, 2):\n",
        "      for dy in range(-2, 2):\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if 0 <= nx < 10 and 0 <= ny < 10:\n",
        "                counter += 1\n",
        "                #relative position to the goal\n",
        "                if env[nx, ny] == TARGET:\n",
        "                  xg,yg = nx, ny\n",
        "                  # action = ACTION_SPACE((xg-x, yg-y))\n",
        "\n",
        "                #leader can see follower\n",
        "                if env[nx,ny] == FOLLOWER:\n",
        "                  follower_visibility = 1\n",
        "                  follower_dist = np.sqrt((x - nx)**2 + (y - ny)**2)\n",
        "\n",
        "                #nearest obstacle\n",
        "                if env[nx, ny] in [OBSTACLE_SOFT, OBSTACLE_HARD]:\n",
        "                    obstacles_pos.append(nx,ny)\n",
        "                    dist = np.sqrt((x - nx)**2 + (y - ny)**2)\n",
        "                    distances.append(dist)\n",
        "\n",
        "  if len(distances) > 0:\n",
        "    obs_dist = min(distances)\n",
        "  if len(obstacles_pos) == counter:\n",
        "      path_blocked = 1\n",
        "  # if action == ACTION_SPACE.STAY.value:\n",
        "  #   if len(obstacles_pos) == counter:\n",
        "  #     path_blocked = 1\n",
        "    # else:\n",
        "    #   while action in obstacles_pos or env[action] == 3:\n",
        "    #     random_actions = random.sample(list(ACTION_SPACE.value), 1)[0]\n",
        "    #     if  0 <= x+random_actions.value[0] < 10 and  0 <= y+random_actions.value[1] < 10:\n",
        "    #         action = random_actions\n",
        "\n",
        "\n",
        "  return [xg, yg, obs_dist, follower_visibility, follower_dist, path_blocked]\n"
      ],
      "metadata": {
        "id": "pUEuOgwtzNVN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "def build_encoder_decoder():\n",
        "    input_layer = Input(shape=(8,))\n",
        "    reshaped = Reshape((1, 8))(input_layer)\n",
        "    x = LSTM(64, return_sequences=True)(reshaped)\n",
        "    x = LSTM(32)(x)\n",
        "    output_layer = Dense(8, activation=\"linear\")(x)\n",
        "    return Model(input_layer, output_layer)\n",
        "\n",
        "encoder_decoder = build_encoder_decoder()"
      ],
      "metadata": {
        "id": "_Il4Q2tHBktj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP MAPPO\n",
        "def build_policy_network():\n",
        "    input_layer = Input(shape=(8,))\n",
        "    x = Dense(64, activation=\"relu\")(input_layer)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    output_layer = Dense(len(ACTION_SPACE), activation=\"softmax\")(x)\n",
        "    return Model(input_layer, output_layer)\n",
        "\n",
        "leader_policy = build_policy_network()\n",
        "follower_policy = build_policy_network()"
      ],
      "metadata": {
        "id": "oSCNcCWsgPJe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAPPO:\n",
        "    def __init__(self, leader_model, follower_model, encoded_model, lr=0.001):\n",
        "        self.leader_model = leader_model\n",
        "        self.follower_model = follower_model\n",
        "        self.encoded_model = encoded_model\n",
        "        self.optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "    def compute_loss(self, state_leader, decoded_msg, action_leader, action_follower, reward, leader_message, encoded_message, decoded_message):\n",
        "        # Compute Advantage (A = R + γV(s') - V(s))\n",
        "        value = self.leader_model(state_leader.reshape(1, -1))[0, 0]  # Predicted value\n",
        "        advantage = reward - value  # TD error as Advantage Estimate\n",
        "        print(\"loss\")\n",
        "\n",
        "        # Policy Gradient Loss (A2C)\n",
        "        action_prob_leader = self.leader_model(state_leader.reshape(1, -1))\n",
        "        action_prob_follower = self.follower_model(decoded_msg.reshape(1, -1))\n",
        "        policy_loss = -tf.reduce_mean(advantage * tf.math.log(action_prob_leader + 1e-8))\n",
        "        print('Policy Gradient Loss')\n",
        "        # Contrastive Loss (CACL) for Communication Alignment\n",
        "        contrastive_loss_value = contrastive_loss(tf.convert_to_tensor([encoded_message]), positive_pairs=[0])\n",
        "\n",
        "        print('Contrastive Loss')\n",
        "        # Message Reconstruction Loss (L_recon)\n",
        "        print(f'leader_message={leader_message}')\n",
        "        print(f'decoded_message= {decoded_message}')\n",
        "        reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(leader_message, decoded_message))\n",
        "        print('Entropy')\n",
        "        # Entropy Bonus for Exploration\n",
        "        entropy_bonus = -tf.reduce_mean(action_prob_leader * tf.math.log(action_prob_leader + 1e-8))\n",
        "\n",
        "        # Final loss function\n",
        "        total_loss = policy_loss + 0.01 * entropy_bonus + 0.5 * contrastive_loss_value + 0.2 * reconstruction_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def apply_gradients(self, state_leader, decoded_msg, action_leader, action_follower, reward):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(state_leader, decoded_msg, action_leader, action_follower, reward)\n",
        "        grads = tape.gradient(loss, self.leader_model.trainable_variables + self.follower_model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.leader_model.trainable_variables + self.follower_model.trainable_variables))"
      ],
      "metadata": {
        "id": "1849kJOJgWJV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# Contrastive Learning for Communication\n",
        "# =======================\n",
        "def contrastive_loss(messages, positive_pairs, temperature=0.1):\n",
        "    messages = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(messages)\n",
        "    sim_matrix = tf.matmul(messages, messages, transpose_b=True) / temperature\n",
        "    labels = tf.one_hot(positive_pairs, depth=len(messages))\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, sim_matrix)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "tKz1iw1bjja6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_MAPPO(episodes, leader_model, follower_model, encoded_model, leader_pos, target_pos,follower_pos):\n",
        "\n",
        "  optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    # reset the environment\n",
        "    env[:] = FREE\n",
        "    env[leader_pos] = LEADER\n",
        "    env[follower_pos] = FOLLOWER\n",
        "    env[target_pos] = TARGET\n",
        "\n",
        "    total_reward = 0\n",
        "    leader_path = [leader_pos]\n",
        "    follower_path = [follower_pos]\n",
        "    episode_reset = False\n",
        "\n",
        "    for step in range(2):\n",
        "\n",
        "        # Leader moves\n",
        "        leader_message = get_leader_message(leader_pos)\n",
        "        leader_message.append(-1)\n",
        "        leader_message.append(-1)\n",
        "        print(\"*********\")\n",
        "        leader_action_probs = leader_model.predict(np.array(leader_message[:8]).reshape(1, -1))\n",
        "        leader_action = list(ACTION_SPACE)[np.argmax(leader_action_probs)]\n",
        "\n",
        "        print('kimia')\n",
        "        leader_message[6], leader_message[7] = leader_action.value\n",
        "        new_leader_pos = new_pos(leader_pos, leader_action)\n",
        "        # new_leader_pos = move_agent(leader_pos, leader_action)\n",
        "        print(leader_action.value)\n",
        "\n",
        "        # encoded decoded\n",
        "        print(len(leader_message))\n",
        "        encoded_msg = encoded_model.predict(np.array(leader_message[:8]).reshape(1, -1))\n",
        "        decoded_msg = encoded_msg.reshape(-1)\n",
        "        print('leader')\n",
        "\n",
        "        # Follower moves\n",
        "        follower_action_probs = follower_model.predict(decoded_msg.reshape(1,-1))\n",
        "        follower_action = list(ACTION_SPACE)[np.argmax(follower_action_probs)]\n",
        "        new_follower_pos = new_pos(follower_pos, follower_action)\n",
        "        print(\"follower\")\n",
        "        # compute distance\n",
        "        distance = np.sqrt((new_leader_pos[0][0] - new_follower_pos[0][0])**2 + (new_leader_pos[0][1] - new_follower_pos[0][1])**2)\n",
        "\n",
        "        x_l , y_l = new_leader_pos[0]\n",
        "        x_f , y_f = new_follower_pos[0]\n",
        "\n",
        "        if distance > 2 or distance < 1:\n",
        "          print(f\"Episode {episode+1}: Distance constraint violated (Distance: {distance:.2f}). Resetting...\")\n",
        "          break\n",
        "\n",
        "\n",
        "        elif env[x_l, y_l] == OBSTACLE_HARD or env[x_f , y_f] == OBSTACLE_HARD:\n",
        "          print(f\"Episode {episode+1}: Hard obstacle constraint violated. Resetting...\")\n",
        "          break\n",
        "\n",
        "        # update the path and position\n",
        "        env[follower_pos] = FREE\n",
        "        follower_pos = new_follower_pos\n",
        "        env[follower_pos] = FOLLOWER\n",
        "        follower_path.append(follower_pos)\n",
        "\n",
        "        env[leader_pos] = FREE\n",
        "        leader_pos = new_leader_pos\n",
        "        env[leader_pos] = LEADER\n",
        "        leader_path.append(leader_pos)\n",
        "\n",
        "\n",
        "        # compute reward\n",
        "        reward = 0\n",
        "        if env[x_l, y_l] == TARGET or env[x_f , y_f] == TARGET:\n",
        "          reward += 10\n",
        "        elif env[x_l, y_l] == OBSTACLE_SOFT or env[x_f , y_f] == OBSTACLE_SOFT:\n",
        "          reward -= 2\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        mappo_model = MAPPO(leader_model, follower_model, encoded_model)\n",
        "        print(\"mappo\")\n",
        "        with tf.GradientTape() as tape:\n",
        "                loss = mappo_model.compute_loss(\n",
        "                    np.array(leader_message[:8]), decoded_msg,\n",
        "                    leader_action, follower_action, reward,\n",
        "                    leader_message, encoded_msg, decoded_msg\n",
        "                )\n",
        "\n",
        "        # Update Policy\n",
        "        print(\"Update Policy\")\n",
        "        grads = tape.gradient(loss, leader_model.trainable_variables + follower_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, leader_model.trainable_variables + follower_model.trainable_variables))\n",
        "\n",
        "\n",
        "  if not episode_reset:\n",
        "            print(f\"\\nEpisode {episode+1} finished with Reward: {total_reward}\")\n",
        "            print(f\"Leader Path: {leader_path}\")\n",
        "            print(f\"Follower Path: {follower_path}\\n\")"
      ],
      "metadata": {
        "id": "TDYDUaj-dVPn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = np.array([\n",
        "    [0, 0, 5, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 3, 4, 0, 0, 0, 0, 1, 1],\n",
        "    [0, 0, 0, 0, 0, 0, 2, 0, 0, 1],\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
        "    [0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 2, 0],\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 2, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 2, 0, 0]\n",
        "  ])\n",
        "\n",
        "leader_pos= np.argwhere(env == LEADER)\n",
        "follower_pos= np.argwhere(env == FOLLOWER)\n",
        "target_pos = np.argwhere(env == TARGET)\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "print(leader_pos)\n",
        "print(follower_pos)\n",
        "print(target_pos)"
      ],
      "metadata": {
        "id": "e0J0xJhkCsTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066f6c44-cd35-4b3b-8f79-55949905c83b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]]\n",
            "[[0 2]]\n",
            "[[1 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_MAPPO(2, leader_policy, follower_policy, encoder_decoder,leader_pos, target_pos, follower_pos,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4xW_rO7FfYb",
        "outputId": "4945c62d-54bc-4ce2-e08e-7cefd98f4ed6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*********\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "kimia\n",
            "(-1, 1)\n",
            "8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "leader\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "follower\n",
            "mappo\n",
            "loss\n",
            "Policy Gradient Loss\n",
            "Contrastive Loss\n",
            "leader_message=[np.int64(1), np.int64(3), -1, 1, np.float64(1.4142135623730951), 0, -1, 1]\n",
            "decoded_message= [ 0.0171809  -0.02600081  0.03360142  0.00800042  0.03094879 -0.05869333\n",
            " -0.02111227  0.00025879]\n",
            "Entropy\n",
            "Update Policy\n",
            "*********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(1, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['dense_4/kernel', 'dense_4/bias', 'dense_5/kernel', 'dense_5/bias', 'dense_6/kernel', 'dense_6/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "kimia\n",
            "(-1, 1)\n",
            "8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "leader\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "follower\n",
            "mappo\n",
            "loss\n",
            "Policy Gradient Loss\n",
            "Contrastive Loss\n",
            "leader_message=[-1, -1, -1, 1, np.float64(1.4142135623730951), 0, -1, 1]\n",
            "decoded_message= [ 0.00652463 -0.012621    0.00657111 -0.01526564 -0.00891721 -0.02557101\n",
            " -0.01264657  0.00010886]\n",
            "Entropy\n",
            "Update Policy\n",
            "*********\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "kimia\n",
            "(-1, 1)\n",
            "8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "leader\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "follower\n",
            "mappo\n",
            "loss\n",
            "Policy Gradient Loss\n",
            "Contrastive Loss\n",
            "leader_message=[np.int64(1), np.int64(3), -1, 1, np.float64(1.4142135623730951), 0, -1, 1]\n",
            "decoded_message= [ 0.0171809  -0.02600081  0.03360142  0.00800042  0.03094879 -0.05869333\n",
            " -0.02111227  0.00025879]\n",
            "Entropy\n",
            "Update Policy\n",
            "*********\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "kimia\n",
            "(-1, 1)\n",
            "8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "leader\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "follower\n",
            "mappo\n",
            "loss\n",
            "Policy Gradient Loss\n",
            "Contrastive Loss\n",
            "leader_message=[-1, -1, -1, 1, np.float64(1.4142135623730951), 0, -1, 1]\n",
            "decoded_message= [ 0.00652463 -0.012621    0.00657111 -0.01526564 -0.00891721 -0.02557101\n",
            " -0.01264657  0.00010886]\n",
            "Entropy\n",
            "Update Policy\n",
            "\n",
            "Episode 2 finished with Reward: 0\n",
            "Leader Path: [array([[1, 2]]), array([[1, 2]]), array([[1, 2]])]\n",
            "Follower Path: [array([[0, 2]]), array([[0, 2]]), array([[0, 2]])]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = leader_pos[0]\n",
        "env[x,y]\n"
      ],
      "metadata": {
        "id": "0t6HO9uQO5yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lr64rHKQO6SZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}